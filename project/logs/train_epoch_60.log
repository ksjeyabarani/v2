Printing shape of training dataset
Iteration #5 loss: 1.4585514068603516
Iteration #10 loss: 1.1687278747558594
Iteration #15 loss: 1.160224437713623
Iteration #20 loss: 1.0747156143188477
Iteration #25 loss: 0.8729196786880493
Iteration #30 loss: 0.8208021521568298
Iteration #35 loss: 0.8903599977493286
Iteration #40 loss: 0.8427044153213501
Iteration #45 loss: 0.9708442687988281
Iteration #50 loss: 0.9007561206817627
Iteration #55 loss: 0.8387778401374817
Iteration #60 loss: 0.8309630751609802
Iteration #65 loss: 0.7966033816337585
Iteration #70 loss: 0.8081061244010925
Iteration #75 loss: 0.7925183773040771
Iteration #80 loss: 0.7740254998207092
Iteration #85 loss: 0.5595055818557739
Iteration #90 loss: 1.2476423978805542
Iteration #95 loss: 0.964504599571228
Iteration #100 loss: 0.7002524137496948
Iteration #105 loss: 0.6792901754379272
Iteration #110 loss: 0.5888919234275818
Iteration #115 loss: 0.7038537859916687
Iteration #120 loss: 0.6481077075004578
Iteration #125 loss: 0.6657615900039673
Iteration #130 loss: 0.6052533388137817
Iteration #135 loss: 0.547086775302887
Iteration #140 loss: 0.7680291533470154
Iteration #145 loss: 0.7326135635375977
Iteration #150 loss: 0.6162534356117249
Iteration #155 loss: 0.7505207657814026
Iteration #160 loss: 0.7081618309020996
Iteration #165 loss: 0.6746457815170288
Iteration #170 loss: 0.6494707465171814
Epoch #0 Training loss: 0.8541380826164694
Iteration #175 loss: 0.7983386516571045
Iteration #180 loss: 0.6710785627365112
Iteration #185 loss: 0.8869925737380981
Iteration #190 loss: 0.8157298564910889
Iteration #195 loss: 0.739181637763977
Iteration #200 loss: 0.7205877304077148
Iteration #205 loss: 0.7194527387619019
Iteration #210 loss: 0.7338089942932129
Iteration #215 loss: 0.8565554618835449
Iteration #220 loss: 0.8434839844703674
Iteration #225 loss: 0.7945185899734497
Iteration #230 loss: 0.7525355815887451
Iteration #235 loss: 0.7344749569892883
Iteration #240 loss: 0.7566614747047424
Iteration #245 loss: 0.7256919741630554
Iteration #250 loss: 0.6717804074287415
Iteration #255 loss: 0.47985735535621643
Iteration #260 loss: 1.0537972450256348
Iteration #265 loss: 0.7994527220726013
Iteration #270 loss: 0.6700305342674255
Iteration #275 loss: 0.6204173564910889
Iteration #280 loss: 0.5367141962051392
Iteration #285 loss: 0.6315401196479797
Iteration #290 loss: 0.6002604961395264
Iteration #295 loss: 0.5918794274330139
Iteration #300 loss: 0.5699403285980225
Iteration #305 loss: 0.5023156404495239
Iteration #310 loss: 0.7043498754501343
Iteration #315 loss: 0.6744023561477661
Iteration #320 loss: 0.6009000539779663
Iteration #325 loss: 0.7837256789207458
Iteration #330 loss: 0.6929832696914673
Iteration #335 loss: 0.6189541816711426
Iteration #340 loss: 0.5957975387573242
Epoch #1 Training loss: 0.7131455968408024
Iteration #345 loss: 0.7289313077926636
Iteration #350 loss: 0.6403704881668091
Iteration #355 loss: 0.9109077453613281
Iteration #360 loss: 0.8078380823135376
Iteration #365 loss: 0.705573558807373
Iteration #370 loss: 0.7202879786491394
Iteration #375 loss: 0.7168221473693848
Iteration #380 loss: 0.7040435671806335
Iteration #385 loss: 0.8260534405708313
Iteration #390 loss: 0.8156582117080688
Iteration #395 loss: 0.7899122834205627
Iteration #400 loss: 0.735418438911438
Iteration #405 loss: 0.7291198968887329
Iteration #410 loss: 0.7447888851165771
Iteration #415 loss: 0.7132283449172974
Iteration #420 loss: 0.6839497089385986
Iteration #425 loss: 0.46121376752853394
Iteration #430 loss: 1.026534914970398
Iteration #435 loss: 0.8476986885070801
Iteration #440 loss: 0.6120730042457581
Iteration #445 loss: 0.6197395324707031
Iteration #450 loss: 0.548570990562439
Iteration #455 loss: 0.616046667098999
Iteration #460 loss: 0.5729174613952637
Iteration #465 loss: 0.6127644181251526
Iteration #470 loss: 0.5800346732139587
Iteration #475 loss: 0.5099354982376099
Iteration #480 loss: 0.7112545967102051
Iteration #485 loss: 0.6218169927597046
Iteration #490 loss: 0.5581806898117065
Iteration #495 loss: 0.750185489654541
Iteration #500 loss: 0.6622596383094788
Iteration #505 loss: 0.5995327234268188
Iteration #510 loss: 0.5528714656829834
Epoch #2 Training loss: 0.6946475025485543
Iteration #515 loss: 0.7221711277961731
Iteration #520 loss: 0.621735155582428
Iteration #525 loss: 0.8923215270042419
Iteration #530 loss: 0.7885681986808777
Iteration #535 loss: 0.6837471723556519
Iteration #540 loss: 0.6991817951202393
Iteration #545 loss: 0.702462911605835
Iteration #550 loss: 0.6975845694541931
Iteration #555 loss: 0.791412353515625
Iteration #560 loss: 0.8005740642547607
Iteration #565 loss: 0.7542827129364014
Iteration #570 loss: 0.7315117716789246
Iteration #575 loss: 0.6989853978157043
Iteration #580 loss: 0.7651224136352539
Iteration #585 loss: 0.7020308971405029
Iteration #590 loss: 0.6695082187652588
Iteration #595 loss: 0.45318400859832764
Iteration #600 loss: 1.0164451599121094
Iteration #605 loss: 0.8373365998268127
Iteration #610 loss: 0.6238691210746765
Iteration #615 loss: 0.6061241030693054
Iteration #620 loss: 0.5510380268096924
Iteration #625 loss: 0.5999574661254883
Iteration #630 loss: 0.5591143369674683
Iteration #635 loss: 0.5816859006881714
Iteration #640 loss: 0.5632635354995728
Iteration #645 loss: 0.4949394762516022
Iteration #650 loss: 0.7227191925048828
Iteration #655 loss: 0.6254878044128418
Iteration #660 loss: 0.5572510361671448
Iteration #665 loss: 0.7243537902832031
Iteration #670 loss: 0.6405342817306519
Iteration #675 loss: 0.5829509496688843
Iteration #680 loss: 0.5650483965873718
Epoch #3 Training loss: 0.6808505745495067
Iteration #685 loss: 0.7395487427711487
Iteration #690 loss: 0.6214508414268494
Iteration #695 loss: 0.8584661483764648
Iteration #700 loss: 0.7794589996337891
Iteration #705 loss: 0.6634699106216431
Iteration #710 loss: 0.7062540650367737
Iteration #715 loss: 0.6792351007461548
Iteration #720 loss: 0.6843452453613281
Iteration #725 loss: 0.7831605076789856
Iteration #730 loss: 0.7925690412521362
Iteration #735 loss: 0.7469142079353333
Iteration #740 loss: 0.7153333425521851
Iteration #745 loss: 0.6950424313545227
Iteration #750 loss: 0.7412160038948059
Iteration #755 loss: 0.7066049575805664
Iteration #760 loss: 0.6495132446289062
Iteration #765 loss: 0.4328988194465637
Iteration #770 loss: 0.9942852258682251
Iteration #775 loss: 0.7755743265151978
Iteration #780 loss: 0.6524595022201538
Iteration #785 loss: 0.5586466193199158
Iteration #790 loss: 0.5409281849861145
Iteration #795 loss: 0.6235866546630859
Iteration #800 loss: 0.5653506517410278
Iteration #805 loss: 0.5724440217018127
Iteration #810 loss: 0.563391387462616
Iteration #815 loss: 0.4859674572944641
Iteration #820 loss: 0.7146902084350586
Iteration #825 loss: 0.6177539825439453
Iteration #830 loss: 0.5438125133514404
Iteration #835 loss: 0.712679922580719
Iteration #840 loss: 0.6176801919937134
Iteration #845 loss: 0.5638953447341919
Iteration #850 loss: 0.5563235282897949
Epoch #4 Training loss: 0.6726427218493294
Iteration #855 loss: 0.7176553010940552
Iteration #860 loss: 0.5911420583724976
Iteration #865 loss: 0.8583678603172302
Iteration #870 loss: 0.7825084924697876
Iteration #875 loss: 0.6636491417884827
Iteration #880 loss: 0.6842865943908691
Iteration #885 loss: 0.6676723957061768
Iteration #890 loss: 0.6627475023269653
Iteration #895 loss: 0.7796813249588013
Iteration #900 loss: 0.785324215888977
Iteration #905 loss: 0.7394430637359619
Iteration #910 loss: 0.707546055316925
Iteration #915 loss: 0.6992257833480835
Iteration #920 loss: 0.7433292865753174
Iteration #925 loss: 0.7183343172073364
Iteration #930 loss: 0.6226122379302979
Iteration #935 loss: 0.4292747378349304
Iteration #940 loss: 0.9486271142959595
Iteration #945 loss: 0.7683917284011841
Iteration #950 loss: 0.6695648431777954
Iteration #955 loss: 0.5407953858375549
Iteration #960 loss: 0.5491739511489868
Iteration #965 loss: 0.6265473365783691
Iteration #970 loss: 0.5617786645889282
Iteration #975 loss: 0.5829720497131348
Iteration #980 loss: 0.5584574937820435
Iteration #985 loss: 0.47380489110946655
Iteration #990 loss: 0.724094033241272
Iteration #995 loss: 0.6053003668785095
Iteration #1000 loss: 0.5314096212387085
Iteration #1005 loss: 0.7051869630813599
Iteration #1010 loss: 0.5859652161598206
Iteration #1015 loss: 0.5621550679206848
Iteration #1020 loss: 0.49896565079689026
Epoch #5 Training loss: 0.6630528255420572
Iteration #1025 loss: 0.6925410628318787
Iteration #1030 loss: 0.626110315322876
Iteration #1035 loss: 0.8769676089286804
Iteration #1040 loss: 0.7680243253707886
Iteration #1045 loss: 0.6571344137191772
Iteration #1050 loss: 0.6974985003471375
Iteration #1055 loss: 0.6584588885307312
Iteration #1060 loss: 0.6736850738525391
Iteration #1065 loss: 0.7746814489364624
Iteration #1070 loss: 0.7678364515304565
Iteration #1075 loss: 0.7395806312561035
Iteration #1080 loss: 0.7107057571411133
Iteration #1085 loss: 0.6838507652282715
Iteration #1090 loss: 0.7367585897445679
Iteration #1095 loss: 0.6840745210647583
Iteration #1100 loss: 0.6194383502006531
Iteration #1105 loss: 0.42673808336257935
Iteration #1110 loss: 0.9582491517066956
Iteration #1115 loss: 0.738797664642334
Iteration #1120 loss: 0.6657801270484924
Iteration #1125 loss: 0.5455400347709656
Iteration #1130 loss: 0.5294122695922852
Iteration #1135 loss: 0.6259280443191528
Iteration #1140 loss: 0.5753309726715088
Iteration #1145 loss: 0.5957614183425903
Iteration #1150 loss: 0.5389646291732788
Iteration #1155 loss: 0.49235817790031433
Iteration #1160 loss: 0.7124678492546082
Iteration #1165 loss: 0.6004895567893982
Iteration #1170 loss: 0.5220510363578796
Iteration #1175 loss: 0.6919026374816895
Iteration #1180 loss: 0.6117992401123047
Iteration #1185 loss: 0.5659421682357788
Iteration #1190 loss: 0.47164440155029297
Epoch #6 Training loss: 0.6587289038826437
Iteration #1195 loss: 0.6781980991363525
Iteration #1200 loss: 0.6029096841812134
Iteration #1205 loss: 0.8603054285049438
Iteration #1210 loss: 0.7725326418876648
Iteration #1215 loss: 0.6489934325218201
Iteration #1220 loss: 0.6893521547317505
Iteration #1225 loss: 0.6673243045806885
Iteration #1230 loss: 0.660656213760376
Iteration #1235 loss: 0.7845112085342407
Iteration #1240 loss: 0.77643883228302
Iteration #1245 loss: 0.7354169487953186
Iteration #1250 loss: 0.6924200654029846
Iteration #1255 loss: 0.67727130651474
Iteration #1260 loss: 0.728884756565094
Iteration #1265 loss: 0.6939070224761963
Iteration #1270 loss: 0.6176100969314575
Iteration #1275 loss: 0.4266546964645386
Iteration #1280 loss: 0.9065333008766174
Iteration #1285 loss: 0.7414007186889648
Iteration #1290 loss: 0.6680482625961304
Iteration #1295 loss: 0.5385700464248657
Iteration #1300 loss: 0.5086021423339844
Iteration #1305 loss: 0.6244174242019653
Iteration #1310 loss: 0.5533027052879333
Iteration #1315 loss: 0.5735940337181091
Iteration #1320 loss: 0.5386661887168884
Iteration #1325 loss: 0.4752505421638489
Iteration #1330 loss: 0.7099775075912476
Iteration #1335 loss: 0.5950492024421692
Iteration #1340 loss: 0.5127229690551758
Iteration #1345 loss: 0.6793390512466431
Iteration #1350 loss: 0.6084240674972534
Iteration #1355 loss: 0.5547816753387451
Iteration #1360 loss: 0.47864824533462524
Epoch #7 Training loss: 0.6518667455981759
Iteration #1365 loss: 0.6893100738525391
Iteration #1370 loss: 0.6046141386032104
Iteration #1375 loss: 0.8285504579544067
Iteration #1380 loss: 0.7614138126373291
Iteration #1385 loss: 0.6390867233276367
Iteration #1390 loss: 0.6821984052658081
Iteration #1395 loss: 0.6362361907958984
Iteration #1400 loss: 0.6542323231697083
Iteration #1405 loss: 0.7678089141845703
Iteration #1410 loss: 0.7731069326400757
Iteration #1415 loss: 0.7095873355865479
Iteration #1420 loss: 0.6998687982559204
Iteration #1425 loss: 0.6743518710136414
Iteration #1430 loss: 0.7503545880317688
Iteration #1435 loss: 0.6874713897705078
Iteration #1440 loss: 0.6064183115959167
Iteration #1445 loss: 0.4170914590358734
Iteration #1450 loss: 0.882635235786438
Iteration #1455 loss: 0.7270485758781433
Iteration #1460 loss: 0.638798713684082
Iteration #1465 loss: 0.5467024445533752
Iteration #1470 loss: 0.49721860885620117
Iteration #1475 loss: 0.620082676410675
Iteration #1480 loss: 0.5553730726242065
Iteration #1485 loss: 0.5617239475250244
Iteration #1490 loss: 0.5309785604476929
Iteration #1495 loss: 0.4702891409397125
Iteration #1500 loss: 0.6859519481658936
Iteration #1505 loss: 0.592826783657074
Iteration #1510 loss: 0.5058257579803467
Iteration #1515 loss: 0.6524421572685242
Iteration #1520 loss: 0.6052221059799194
Iteration #1525 loss: 0.5341273546218872
Iteration #1530 loss: 0.49307286739349365
Epoch #8 Training loss: 0.6433490058955025
Iteration #1535 loss: 0.6802637577056885
Iteration #1540 loss: 0.5759972929954529
Iteration #1545 loss: 0.8409377932548523
Iteration #1550 loss: 0.7425146102905273
Iteration #1555 loss: 0.636617124080658
Iteration #1560 loss: 0.6744065284729004
Iteration #1565 loss: 0.6362304091453552
Iteration #1570 loss: 0.6530952453613281
Iteration #1575 loss: 0.7527605891227722
Iteration #1580 loss: 0.7720116376876831
Iteration #1585 loss: 0.728303074836731
Iteration #1590 loss: 0.6902123093605042
Iteration #1595 loss: 0.6740745306015015
Iteration #1600 loss: 0.726333498954773
Iteration #1605 loss: 0.6723220348358154
Iteration #1610 loss: 0.584925651550293
Iteration #1615 loss: 0.4079834818840027
Iteration #1620 loss: 0.8619776964187622
Iteration #1625 loss: 0.7099791765213013
Iteration #1630 loss: 0.60728520154953
Iteration #1635 loss: 0.5557971596717834
Iteration #1640 loss: 0.4834710657596588
Iteration #1645 loss: 0.6042886972427368
Iteration #1650 loss: 0.5713014006614685
Iteration #1655 loss: 0.568544864654541
Iteration #1660 loss: 0.5353454351425171
Iteration #1665 loss: 0.45935747027397156
Iteration #1670 loss: 0.6824715733528137
Iteration #1675 loss: 0.564863920211792
Iteration #1680 loss: 0.4890116751194
Iteration #1685 loss: 0.669291615486145
Iteration #1690 loss: 0.6121933460235596
Iteration #1695 loss: 0.5160131454467773
Iteration #1700 loss: 0.4660915732383728
Epoch #9 Training loss: 0.6351674344609766
Iteration #1705 loss: 0.6883898973464966
Iteration #1710 loss: 0.5819605588912964
Iteration #1715 loss: 0.8173370957374573
Iteration #1720 loss: 0.7394914627075195
Iteration #1725 loss: 0.6343193054199219
Iteration #1730 loss: 0.6706151366233826
Iteration #1735 loss: 0.6400892734527588
Iteration #1740 loss: 0.6499694585800171
Iteration #1745 loss: 0.750208854675293
Iteration #1750 loss: 0.7493396997451782
Iteration #1755 loss: 0.6969103813171387
Iteration #1760 loss: 0.6883653402328491
Iteration #1765 loss: 0.658077597618103
Iteration #1770 loss: 0.7145696878433228
Iteration #1775 loss: 0.6834303140640259
Iteration #1780 loss: 0.5947084426879883
Iteration #1785 loss: 0.39718639850616455
Iteration #1790 loss: 0.8389572501182556
Iteration #1795 loss: 0.7106053829193115
Iteration #1800 loss: 0.595594048500061
Iteration #1805 loss: 0.5395750999450684
Iteration #1810 loss: 0.4675225615501404
Iteration #1815 loss: 0.6080060005187988
Iteration #1820 loss: 0.5520209074020386
Iteration #1825 loss: 0.5524218678474426
Iteration #1830 loss: 0.5251372456550598
Iteration #1835 loss: 0.4617607593536377
Iteration #1840 loss: 0.6664372682571411
Iteration #1845 loss: 0.5740927457809448
Iteration #1850 loss: 0.5053166151046753
Iteration #1855 loss: 0.6665002107620239
Iteration #1860 loss: 0.5968564748764038
Iteration #1865 loss: 0.5402039885520935
Iteration #1870 loss: 0.46463441848754883
Epoch #10 Training loss: 0.6307977523873834
Iteration #1875 loss: 0.6842926740646362
Iteration #1880 loss: 0.5740196704864502
Iteration #1885 loss: 0.8290151953697205
Iteration #1890 loss: 0.7481497526168823
Iteration #1895 loss: 0.6179179549217224
Iteration #1900 loss: 0.6598581075668335
Iteration #1905 loss: 0.6421103477478027
Iteration #1910 loss: 0.6340367197990417
Iteration #1915 loss: 0.7546440362930298
Iteration #1920 loss: 0.7499566078186035
Iteration #1925 loss: 0.7161065936088562
Iteration #1930 loss: 0.6749638319015503
Iteration #1935 loss: 0.6651736497879028
Iteration #1940 loss: 0.7030916213989258
Iteration #1945 loss: 0.6791603565216064
Iteration #1950 loss: 0.5830235481262207
Iteration #1955 loss: 0.40397781133651733
Iteration #1960 loss: 0.80824875831604
Iteration #1965 loss: 0.6854761838912964
Iteration #1970 loss: 0.5979616045951843
Iteration #1975 loss: 0.5362027883529663
Iteration #1980 loss: 0.4590516984462738
Iteration #1985 loss: 0.5829761028289795
Iteration #1990 loss: 0.540381669998169
Iteration #1995 loss: 0.5421228408813477
Iteration #2000 loss: 0.5278525352478027
Iteration #2005 loss: 0.4505109190940857
Iteration #2010 loss: 0.6793925762176514
Iteration #2015 loss: 0.5740153789520264
Iteration #2020 loss: 0.4668552875518799
Iteration #2025 loss: 0.6537227034568787
Iteration #2030 loss: 0.5840449333190918
Iteration #2035 loss: 0.5503426790237427
Iteration #2040 loss: 0.48405635356903076
Epoch #11 Training loss: 0.6247591826845618
Iteration #2045 loss: 0.6553058624267578
Iteration #2050 loss: 0.5851256847381592
Iteration #2055 loss: 0.798729419708252
Iteration #2060 loss: 0.7402725219726562
Iteration #2065 loss: 0.6145756840705872
Iteration #2070 loss: 0.6675365567207336
Iteration #2075 loss: 0.645066499710083
Iteration #2080 loss: 0.6427605748176575
Iteration #2085 loss: 0.7647895216941833
Iteration #2090 loss: 0.7536964416503906
Iteration #2095 loss: 0.7077231407165527
Iteration #2100 loss: 0.6682083606719971
Iteration #2105 loss: 0.684220016002655
Iteration #2110 loss: 0.702039361000061
Iteration #2115 loss: 0.6745522022247314
Iteration #2120 loss: 0.5663020014762878
Iteration #2125 loss: 0.3843129277229309
Iteration #2130 loss: 0.7954721450805664
Iteration #2135 loss: 0.6795938014984131
Iteration #2140 loss: 0.5952632427215576
Iteration #2145 loss: 0.5210795402526855
Iteration #2150 loss: 0.4556598961353302
Iteration #2155 loss: 0.5756509900093079
Iteration #2160 loss: 0.5396559238433838
Iteration #2165 loss: 0.5228434205055237
Iteration #2170 loss: 0.520892858505249
Iteration #2175 loss: 0.45349007844924927
Iteration #2180 loss: 0.6592573523521423
Iteration #2185 loss: 0.5605801343917847
Iteration #2190 loss: 0.4902709722518921
Iteration #2195 loss: 0.6430317759513855
Iteration #2200 loss: 0.5899412631988525
Iteration #2205 loss: 0.5127838850021362
Iteration #2210 loss: 0.4044265151023865
Epoch #12 Training loss: 0.6188152886488858
Iteration #2215 loss: 0.6801505088806152
Iteration #2220 loss: 0.5752192139625549
Iteration #2225 loss: 0.7934237718582153
Iteration #2230 loss: 0.7218162417411804
Iteration #2235 loss: 0.6165826320648193
Iteration #2240 loss: 0.6617259383201599
Iteration #2245 loss: 0.6233181953430176
Iteration #2250 loss: 0.6257601976394653
Iteration #2255 loss: 0.742658257484436
Iteration #2260 loss: 0.7546771764755249
Iteration #2265 loss: 0.7045220136642456
Iteration #2270 loss: 0.6742488145828247
Iteration #2275 loss: 0.6475424766540527
Iteration #2280 loss: 0.7006728053092957
Iteration #2285 loss: 0.6708323955535889
Iteration #2290 loss: 0.561130166053772
Iteration #2295 loss: 0.3942698836326599
Iteration #2300 loss: 0.7711575627326965
Iteration #2305 loss: 0.6713417172431946
Iteration #2310 loss: 0.5816431045532227
Iteration #2315 loss: 0.5232073664665222
Iteration #2320 loss: 0.4591262936592102
Iteration #2325 loss: 0.5666587352752686
Iteration #2330 loss: 0.5198314189910889
Iteration #2335 loss: 0.5421969890594482
Iteration #2340 loss: 0.5195684432983398
Iteration #2345 loss: 0.4625874161720276
Iteration #2350 loss: 0.6669561862945557
Iteration #2355 loss: 0.5758870244026184
Iteration #2360 loss: 0.4603738784790039
Iteration #2365 loss: 0.6441869735717773
Iteration #2370 loss: 0.599504292011261
Iteration #2375 loss: 0.5392271280288696
Iteration #2380 loss: 0.4214228391647339
Epoch #13 Training loss: 0.6152344514341915
Iteration #2385 loss: 0.66175776720047
Iteration #2390 loss: 0.5662232637405396
Iteration #2395 loss: 0.7995250225067139
Iteration #2400 loss: 0.7258440256118774
Iteration #2405 loss: 0.610666036605835
Iteration #2410 loss: 0.6504947543144226
Iteration #2415 loss: 0.6240088939666748
Iteration #2420 loss: 0.6367495656013489
Iteration #2425 loss: 0.7378475069999695
Iteration #2430 loss: 0.7465059161186218
Iteration #2435 loss: 0.6976168155670166
Iteration #2440 loss: 0.6735118627548218
Iteration #2445 loss: 0.6431174278259277
Iteration #2450 loss: 0.7045451402664185
Iteration #2455 loss: 0.6806615591049194
Iteration #2460 loss: 0.5343382358551025
Iteration #2465 loss: 0.39410400390625
Iteration #2470 loss: 0.7717576026916504
Iteration #2475 loss: 0.6680553555488586
Iteration #2480 loss: 0.5694500803947449
Iteration #2485 loss: 0.5230073928833008
Iteration #2490 loss: 0.4497939348220825
Iteration #2495 loss: 0.5684312582015991
Iteration #2500 loss: 0.5480900406837463
Iteration #2505 loss: 0.5257208347320557
Iteration #2510 loss: 0.5065113306045532
Iteration #2515 loss: 0.4397490322589874
Iteration #2520 loss: 0.65697181224823
Iteration #2525 loss: 0.5514519810676575
Iteration #2530 loss: 0.48007166385650635
Iteration #2535 loss: 0.6657943725585938
Iteration #2540 loss: 0.5759652853012085
Iteration #2545 loss: 0.5167003870010376
Iteration #2550 loss: 0.4770030975341797
Epoch #14 Training loss: 0.6098036827409968
Iteration #2555 loss: 0.6714977622032166
Iteration #2560 loss: 0.5702743530273438
Iteration #2565 loss: 0.7794089317321777
Iteration #2570 loss: 0.7397311925888062
Iteration #2575 loss: 0.6093846559524536
Iteration #2580 loss: 0.6377048492431641
Iteration #2585 loss: 0.6222373247146606
Iteration #2590 loss: 0.6261672973632812
Iteration #2595 loss: 0.7248056530952454
Iteration #2600 loss: 0.7215806841850281
Iteration #2605 loss: 0.6810109615325928
Iteration #2610 loss: 0.6634733080863953
Iteration #2615 loss: 0.652545690536499
Iteration #2620 loss: 0.7014630436897278
Iteration #2625 loss: 0.6409336924552917
Iteration #2630 loss: 0.535916805267334
Iteration #2635 loss: 0.3862704336643219
Iteration #2640 loss: 0.7709066867828369
Iteration #2645 loss: 0.6597626805305481
Iteration #2650 loss: 0.5657267570495605
Iteration #2655 loss: 0.5133216381072998
Iteration #2660 loss: 0.42452144622802734
Iteration #2665 loss: 0.5462795495986938
Iteration #2670 loss: 0.5229829549789429
Iteration #2675 loss: 0.5344171524047852
Iteration #2680 loss: 0.5130756497383118
Iteration #2685 loss: 0.4315810203552246
Iteration #2690 loss: 0.6659861207008362
Iteration #2695 loss: 0.566628634929657
Iteration #2700 loss: 0.4555216431617737
Iteration #2705 loss: 0.6431615352630615
Iteration #2710 loss: 0.5667644739151001
Iteration #2715 loss: 0.5148325562477112
Iteration #2720 loss: 0.42830607295036316
Epoch #15 Training loss: 0.6040346201728372
Iteration #2725 loss: 0.6612340211868286
Iteration #2730 loss: 0.5672460198402405
Iteration #2735 loss: 0.769550085067749
Iteration #2740 loss: 0.7060308456420898
Iteration #2745 loss: 0.5894222259521484
Iteration #2750 loss: 0.6508898735046387
Iteration #2755 loss: 0.6316034197807312
Iteration #2760 loss: 0.6126517653465271
Iteration #2765 loss: 0.743911623954773
Iteration #2770 loss: 0.7269021272659302
Iteration #2775 loss: 0.6653012037277222
Iteration #2780 loss: 0.6550115942955017
Iteration #2785 loss: 0.6533259749412537
Iteration #2790 loss: 0.7008917927742004
Iteration #2795 loss: 0.6533368825912476
Iteration #2800 loss: 0.5174621343612671
Iteration #2805 loss: 0.3652641773223877
Iteration #2810 loss: 0.7451143264770508
Iteration #2815 loss: 0.6556631922721863
Iteration #2820 loss: 0.5578569173812866
Iteration #2825 loss: 0.5033890604972839
Iteration #2830 loss: 0.42503100633621216
Iteration #2835 loss: 0.5540536046028137
Iteration #2840 loss: 0.5204835534095764
Iteration #2845 loss: 0.5329407453536987
Iteration #2850 loss: 0.5096744894981384
Iteration #2855 loss: 0.43564850091934204
Iteration #2860 loss: 0.6595144271850586
Iteration #2865 loss: 0.5657840967178345
Iteration #2870 loss: 0.4493064284324646
Iteration #2875 loss: 0.6403588652610779
Iteration #2880 loss: 0.5849223136901855
Iteration #2885 loss: 0.51844722032547
Iteration #2890 loss: 0.43539026379585266
Epoch #16 Training loss: 0.5999637221588808
Iteration #2895 loss: 0.6500875949859619
Iteration #2900 loss: 0.556621789932251
Iteration #2905 loss: 0.7559237480163574
Iteration #2910 loss: 0.7143089771270752
Iteration #2915 loss: 0.5942793488502502
Iteration #2920 loss: 0.6239301562309265
Iteration #2925 loss: 0.6221249103546143
Iteration #2930 loss: 0.6228656768798828
Iteration #2935 loss: 0.7183700203895569
Iteration #2940 loss: 0.7180151343345642
Iteration #2945 loss: 0.6850419640541077
Iteration #2950 loss: 0.6615588665008545
Iteration #2955 loss: 0.6217107772827148
Iteration #2960 loss: 0.6998708844184875
Iteration #2965 loss: 0.6648613810539246
Iteration #2970 loss: 0.5279954671859741
Iteration #2975 loss: 0.3619502782821655
Iteration #2980 loss: 0.7355512380599976
Iteration #2985 loss: 0.6354007720947266
Iteration #2990 loss: 0.5528398752212524
Iteration #2995 loss: 0.49752721190452576
Iteration #3000 loss: 0.4256555438041687
Iteration #3005 loss: 0.5472676157951355
Iteration #3010 loss: 0.5247478485107422
Iteration #3015 loss: 0.5273866057395935
Iteration #3020 loss: 0.5128612518310547
Iteration #3025 loss: 0.44128862023353577
Iteration #3030 loss: 0.6587238311767578
Iteration #3035 loss: 0.5507417917251587
Iteration #3040 loss: 0.43294867873191833
Iteration #3045 loss: 0.6510443687438965
Iteration #3050 loss: 0.5595918893814087
Iteration #3055 loss: 0.47900083661079407
Iteration #3060 loss: 0.40894708037376404
Epoch #17 Training loss: 0.5945275801069596
Iteration #3065 loss: 0.628836989402771
Iteration #3070 loss: 0.5631377696990967
Iteration #3075 loss: 0.7402633428573608
Iteration #3080 loss: 0.7101155519485474
Iteration #3085 loss: 0.5926339030265808
Iteration #3090 loss: 0.6259914040565491
Iteration #3095 loss: 0.632098376750946
Iteration #3100 loss: 0.6248658895492554
Iteration #3105 loss: 0.7154303789138794
Iteration #3110 loss: 0.7301057577133179
Iteration #3115 loss: 0.6918842196464539
Iteration #3120 loss: 0.6610298752784729
Iteration #3125 loss: 0.6389552354812622
Iteration #3130 loss: 0.6851001381874084
Iteration #3135 loss: 0.6507719159126282
Iteration #3140 loss: 0.5102624297142029
Iteration #3145 loss: 0.3715994358062744
Iteration #3150 loss: 0.7143816947937012
Iteration #3155 loss: 0.6138723492622375
Iteration #3160 loss: 0.5430819988250732
Iteration #3165 loss: 0.5032156705856323
Iteration #3170 loss: 0.4308614432811737
Iteration #3175 loss: 0.5596495866775513
Iteration #3180 loss: 0.5123636722564697
Iteration #3185 loss: 0.5165749788284302
Iteration #3190 loss: 0.5084445476531982
Iteration #3195 loss: 0.4420962333679199
Iteration #3200 loss: 0.641486406326294
Iteration #3205 loss: 0.5439237952232361
Iteration #3210 loss: 0.42873406410217285
Iteration #3215 loss: 0.6452946662902832
Iteration #3220 loss: 0.5549356937408447
Iteration #3225 loss: 0.49813491106033325
Iteration #3230 loss: 0.40751004219055176
Epoch #18 Training loss: 0.5917978276224697
Iteration #3235 loss: 0.6497867107391357
Iteration #3240 loss: 0.5470958948135376
Iteration #3245 loss: 0.7384542226791382
Iteration #3250 loss: 0.7085528373718262
Iteration #3255 loss: 0.5835526585578918
Iteration #3260 loss: 0.6197468638420105
Iteration #3265 loss: 0.6165475249290466
Iteration #3270 loss: 0.6156290769577026
Iteration #3275 loss: 0.7246716022491455
Iteration #3280 loss: 0.722643256187439
Iteration #3285 loss: 0.673566460609436
Iteration #3290 loss: 0.6554745435714722
Iteration #3295 loss: 0.6391093730926514
Iteration #3300 loss: 0.6942461729049683
Iteration #3305 loss: 0.6621053218841553
Iteration #3310 loss: 0.5014538764953613
Iteration #3315 loss: 0.3579184412956238
Iteration #3320 loss: 0.7160700559616089
Iteration #3325 loss: 0.6174777746200562
Iteration #3330 loss: 0.5357890725135803
Iteration #3335 loss: 0.49192479252815247
Iteration #3340 loss: 0.41037434339523315
Iteration #3345 loss: 0.5473884344100952
Iteration #3350 loss: 0.5131992697715759
Iteration #3355 loss: 0.5369027853012085
Iteration #3360 loss: 0.481597900390625
Iteration #3365 loss: 0.4380771219730377
Iteration #3370 loss: 0.6528675556182861
Iteration #3375 loss: 0.5489057302474976
Iteration #3380 loss: 0.42562875151634216
Iteration #3385 loss: 0.6268037557601929
Iteration #3390 loss: 0.5684729814529419
Iteration #3395 loss: 0.4987097978591919
Iteration #3400 loss: 0.3918154835700989
Epoch #19 Training loss: 0.5880717652685502
Iteration #3405 loss: 0.6427794098854065
Iteration #3410 loss: 0.5431223511695862
Iteration #3415 loss: 0.7407930493354797
Iteration #3420 loss: 0.6852541565895081
Iteration #3425 loss: 0.5825343132019043
Iteration #3430 loss: 0.6174161434173584
Iteration #3435 loss: 0.6028159856796265
Iteration #3440 loss: 0.6126153469085693
Iteration #3445 loss: 0.7012295126914978
Iteration #3450 loss: 0.716421902179718
Iteration #3455 loss: 0.6635183095932007
Iteration #3460 loss: 0.6420187950134277
Iteration #3465 loss: 0.6450541615486145
Iteration #3470 loss: 0.699162483215332
Iteration #3475 loss: 0.6468055248260498
Iteration #3480 loss: 0.5035947561264038
Iteration #3485 loss: 0.36663174629211426
Iteration #3490 loss: 0.7036451697349548
Iteration #3495 loss: 0.6118229627609253
Iteration #3500 loss: 0.5351008772850037
Iteration #3505 loss: 0.4932267665863037
Iteration #3510 loss: 0.41779956221580505
Iteration #3515 loss: 0.5334796905517578
Iteration #3520 loss: 0.5142707228660583
Iteration #3525 loss: 0.5168553590774536
Iteration #3530 loss: 0.493948370218277
Iteration #3535 loss: 0.4309462904930115
Iteration #3540 loss: 0.6374831795692444
Iteration #3545 loss: 0.5375539660453796
Iteration #3550 loss: 0.43574628233909607
Iteration #3555 loss: 0.6260061860084534
Iteration #3560 loss: 0.5647011399269104
Iteration #3565 loss: 0.4889712333679199
Iteration #3570 loss: 0.39838898181915283
Epoch #20 Training loss: 0.583918117425021
Iteration #3575 loss: 0.6531006097793579
Iteration #3580 loss: 0.5408850312232971
Iteration #3585 loss: 0.719875693321228
Iteration #3590 loss: 0.6956417560577393
Iteration #3595 loss: 0.5979530215263367
Iteration #3600 loss: 0.6128888130187988
Iteration #3605 loss: 0.623306393623352
Iteration #3610 loss: 0.6115162372589111
Iteration #3615 loss: 0.7065526843070984
Iteration #3620 loss: 0.7321059703826904
Iteration #3625 loss: 0.6724475026130676
Iteration #3630 loss: 0.6511775255203247
Iteration #3635 loss: 0.6279317140579224
Iteration #3640 loss: 0.6953134536743164
Iteration #3645 loss: 0.6587588787078857
Iteration #3650 loss: 0.492395281791687
Iteration #3655 loss: 0.35391515493392944
Iteration #3660 loss: 0.7086580991744995
Iteration #3665 loss: 0.6029768586158752
Iteration #3670 loss: 0.5392848253250122
Iteration #3675 loss: 0.48271438479423523
Iteration #3680 loss: 0.4098864197731018
Iteration #3685 loss: 0.5229084491729736
Iteration #3690 loss: 0.5113112926483154
Iteration #3695 loss: 0.5130429267883301
Iteration #3700 loss: 0.4934508502483368
Iteration #3705 loss: 0.4385080337524414
Iteration #3710 loss: 0.6550124287605286
Iteration #3715 loss: 0.536934494972229
Iteration #3720 loss: 0.41754910349845886
Iteration #3725 loss: 0.6431772708892822
Iteration #3730 loss: 0.5731128454208374
Iteration #3735 loss: 0.5070297718048096
Iteration #3740 loss: 0.41660869121551514
Epoch #21 Training loss: 0.5847857254392961
Iteration #3745 loss: 0.6316286325454712
Iteration #3750 loss: 0.5386356115341187
Iteration #3755 loss: 0.726793646812439
Iteration #3760 loss: 0.6982769966125488
Iteration #3765 loss: 0.587444543838501
Iteration #3770 loss: 0.6126320362091064
Iteration #3775 loss: 0.6065254807472229
Iteration #3780 loss: 0.5922113656997681
Iteration #3785 loss: 0.6997093558311462
Iteration #3790 loss: 0.7014526724815369
Iteration #3795 loss: 0.6848739385604858
Iteration #3800 loss: 0.6427950859069824
Iteration #3805 loss: 0.6471590995788574
Iteration #3810 loss: 0.6968018412590027
Iteration #3815 loss: 0.6509670615196228
Iteration #3820 loss: 0.49393007159233093
Iteration #3825 loss: 0.3622171878814697
Iteration #3830 loss: 0.7069404125213623
Iteration #3835 loss: 0.6018854379653931
Iteration #3840 loss: 0.5400803089141846
Iteration #3845 loss: 0.49693992733955383
Iteration #3850 loss: 0.4007943868637085
Iteration #3855 loss: 0.5273407697677612
Iteration #3860 loss: 0.5148535966873169
Iteration #3865 loss: 0.5021340250968933
Iteration #3870 loss: 0.4924156963825226
Iteration #3875 loss: 0.4423580765724182
Iteration #3880 loss: 0.6361619830131531
Iteration #3885 loss: 0.534828782081604
Iteration #3890 loss: 0.42060089111328125
Iteration #3895 loss: 0.606877863407135
Iteration #3900 loss: 0.5663349032402039
Iteration #3905 loss: 0.4946289658546448
Iteration #3910 loss: 0.35007768869400024
Epoch #22 Training loss: 0.5787330974550808
Iteration #3915 loss: 0.6461133360862732
Iteration #3920 loss: 0.5325215458869934
Iteration #3925 loss: 0.7272911071777344
Iteration #3930 loss: 0.6779344081878662
Iteration #3935 loss: 0.5988134145736694
Iteration #3940 loss: 0.6182093620300293
Iteration #3945 loss: 0.6178025007247925
Iteration #3950 loss: 0.6029502749443054
Iteration #3955 loss: 0.6997398138046265
Iteration #3960 loss: 0.7025054693222046
Iteration #3965 loss: 0.6509792804718018
Iteration #3970 loss: 0.6439756155014038
Iteration #3975 loss: 0.6275527477264404
Iteration #3980 loss: 0.6817258596420288
Iteration #3985 loss: 0.6469525098800659
Iteration #3990 loss: 0.4708506762981415
Iteration #3995 loss: 0.3577190637588501
Iteration #4000 loss: 0.6920731067657471
Iteration #4005 loss: 0.5892247557640076
Iteration #4010 loss: 0.530091404914856
Iteration #4015 loss: 0.48722708225250244
Iteration #4020 loss: 0.4064517617225647
Iteration #4025 loss: 0.5310143232345581
Iteration #4030 loss: 0.5112375020980835
Iteration #4035 loss: 0.508521318435669
Iteration #4040 loss: 0.474099725484848
Iteration #4045 loss: 0.4167172908782959
Iteration #4050 loss: 0.6243430972099304
Iteration #4055 loss: 0.5409977436065674
Iteration #4060 loss: 0.4287649393081665
Iteration #4065 loss: 0.6134909987449646
Iteration #4070 loss: 0.538129448890686
Iteration #4075 loss: 0.4971243739128113
Iteration #4080 loss: 0.3814893662929535
Epoch #23 Training loss: 0.5764896270106821
Iteration #4085 loss: 0.6048001050949097
Iteration #4090 loss: 0.5245586633682251
Iteration #4095 loss: 0.7097191214561462
Iteration #4100 loss: 0.6908947229385376
Iteration #4105 loss: 0.5885090827941895
Iteration #4110 loss: 0.6131391525268555
Iteration #4115 loss: 0.6186405420303345
Iteration #4120 loss: 0.6090177893638611
Iteration #4125 loss: 0.7096139192581177
Iteration #4130 loss: 0.7055104970932007
Iteration #4135 loss: 0.6386333703994751
Iteration #4140 loss: 0.6347901225090027
Iteration #4145 loss: 0.6355794668197632
Iteration #4150 loss: 0.6850399971008301
Iteration #4155 loss: 0.6407817006111145
Iteration #4160 loss: 0.48395389318466187
Iteration #4165 loss: 0.3519742488861084
Iteration #4170 loss: 0.6835200190544128
Iteration #4175 loss: 0.5834740400314331
Iteration #4180 loss: 0.5211044549942017
Iteration #4185 loss: 0.47159671783447266
Iteration #4190 loss: 0.4073278605937958
Iteration #4195 loss: 0.5106960535049438
Iteration #4200 loss: 0.4973316192626953
Iteration #4205 loss: 0.5173172950744629
Iteration #4210 loss: 0.4937611222267151
Iteration #4215 loss: 0.4239140748977661
Iteration #4220 loss: 0.6323114037513733
Iteration #4225 loss: 0.532102644443512
Iteration #4230 loss: 0.4200277030467987
Iteration #4235 loss: 0.6229335069656372
Iteration #4240 loss: 0.5836684107780457
Iteration #4245 loss: 0.4851979613304138
Iteration #4250 loss: 0.3732711374759674
Epoch #24 Training loss: 0.5740062969572404
Iteration #4255 loss: 0.6226732134819031
Iteration #4260 loss: 0.5256125926971436
Iteration #4265 loss: 0.710800290107727
Iteration #4270 loss: 0.7017514705657959
Iteration #4275 loss: 0.5859376788139343
Iteration #4280 loss: 0.6046126484870911
Iteration #4285 loss: 0.6131660342216492
Iteration #4290 loss: 0.6109544038772583
Iteration #4295 loss: 0.6975680589675903
Iteration #4300 loss: 0.7051492929458618
Iteration #4305 loss: 0.6621613502502441
Iteration #4310 loss: 0.6365844011306763
Iteration #4315 loss: 0.6315615177154541
Iteration #4320 loss: 0.6924806833267212
Iteration #4325 loss: 0.6360995769500732
Iteration #4330 loss: 0.4713486135005951
Iteration #4335 loss: 0.3449540138244629
Iteration #4340 loss: 0.6903660297393799
Iteration #4345 loss: 0.5666646957397461
Iteration #4350 loss: 0.5344777703285217
Iteration #4355 loss: 0.47046640515327454
Iteration #4360 loss: 0.4015551209449768
Iteration #4365 loss: 0.532705545425415
Iteration #4370 loss: 0.5045586228370667
Iteration #4375 loss: 0.4977527856826782
Iteration #4380 loss: 0.49262648820877075
Iteration #4385 loss: 0.4205508232116699
Iteration #4390 loss: 0.6379930973052979
Iteration #4395 loss: 0.5367352962493896
Iteration #4400 loss: 0.41757822036743164
Iteration #4405 loss: 0.6138439774513245
Iteration #4410 loss: 0.5430282354354858
Iteration #4415 loss: 0.4814381003379822
Iteration #4420 loss: 0.4212883710861206
Epoch #25 Training loss: 0.5717885937760858
Iteration #4425 loss: 0.6421089172363281
Iteration #4430 loss: 0.5297796726226807
Iteration #4435 loss: 0.6837993264198303
Iteration #4440 loss: 0.6914273500442505
Iteration #4445 loss: 0.5868948698043823
Iteration #4450 loss: 0.5967433452606201
Iteration #4455 loss: 0.6101016402244568
Iteration #4460 loss: 0.612555205821991
Iteration #4465 loss: 0.6847413182258606
Iteration #4470 loss: 0.7052402496337891
Iteration #4475 loss: 0.6383169889450073
Iteration #4480 loss: 0.6445538997650146
Iteration #4485 loss: 0.6324131488800049
Iteration #4490 loss: 0.679982602596283
Iteration #4495 loss: 0.6300520896911621
Iteration #4500 loss: 0.47453826665878296
Iteration #4505 loss: 0.34387558698654175
Iteration #4510 loss: 0.6736829876899719
Iteration #4515 loss: 0.5690767765045166
Iteration #4520 loss: 0.5291987657546997
Iteration #4525 loss: 0.48005247116088867
Iteration #4530 loss: 0.40632307529449463
Iteration #4535 loss: 0.526141881942749
Iteration #4540 loss: 0.48686814308166504
Iteration #4545 loss: 0.4975528120994568
Iteration #4550 loss: 0.47571122646331787
Iteration #4555 loss: 0.40916335582733154
Iteration #4560 loss: 0.6278858184814453
Iteration #4565 loss: 0.5223853588104248
Iteration #4570 loss: 0.4221932590007782
Iteration #4575 loss: 0.6406736373901367
Iteration #4580 loss: 0.5506739616394043
Iteration #4585 loss: 0.4826936721801758
Iteration #4590 loss: 0.3691253066062927
Epoch #26 Training loss: 0.5692721295006135
Iteration #4595 loss: 0.6137388348579407
Iteration #4600 loss: 0.5328536033630371
Iteration #4605 loss: 0.6960219144821167
Iteration #4610 loss: 0.6874803304672241
Iteration #4615 loss: 0.569373607635498
Iteration #4620 loss: 0.6053982377052307
Iteration #4625 loss: 0.594632089138031
Iteration #4630 loss: 0.5977747440338135
Iteration #4635 loss: 0.6803329586982727
Iteration #4640 loss: 0.6999620199203491
Iteration #4645 loss: 0.6505324244499207
Iteration #4650 loss: 0.6335607767105103
Iteration #4655 loss: 0.6269290447235107
Iteration #4660 loss: 0.6812198758125305
Iteration #4665 loss: 0.634965717792511
Iteration #4670 loss: 0.4700618386268616
Iteration #4675 loss: 0.337721049785614
Iteration #4680 loss: 0.6794736981391907
Iteration #4685 loss: 0.565674901008606
Iteration #4690 loss: 0.5262722373008728
Iteration #4695 loss: 0.4828852415084839
Iteration #4700 loss: 0.3882043957710266
Iteration #4705 loss: 0.5074689388275146
Iteration #4710 loss: 0.4834522604942322
Iteration #4715 loss: 0.5122129917144775
Iteration #4720 loss: 0.4903566837310791
Iteration #4725 loss: 0.42405641078948975
Iteration #4730 loss: 0.6144695281982422
Iteration #4735 loss: 0.5207342505455017
Iteration #4740 loss: 0.4080357849597931
Iteration #4745 loss: 0.6507095098495483
Iteration #4750 loss: 0.5555918216705322
Iteration #4755 loss: 0.4742489457130432
Iteration #4760 loss: 0.3786170482635498
Epoch #27 Training loss: 0.5662271944915547
Iteration #4765 loss: 0.616281270980835
Iteration #4770 loss: 0.5126712918281555
Iteration #4775 loss: 0.6954053640365601
Iteration #4780 loss: 0.6852693557739258
Iteration #4785 loss: 0.5682077407836914
Iteration #4790 loss: 0.6147768497467041
Iteration #4795 loss: 0.6101412177085876
Iteration #4800 loss: 0.6083481907844543
Iteration #4805 loss: 0.7080488204956055
Iteration #4810 loss: 0.7022314071655273
Iteration #4815 loss: 0.636134684085846
Iteration #4820 loss: 0.6189086437225342
Iteration #4825 loss: 0.6189706325531006
Iteration #4830 loss: 0.668709933757782
Iteration #4835 loss: 0.6305701732635498
Iteration #4840 loss: 0.46050816774368286
Iteration #4845 loss: 0.3424425423145294
Iteration #4850 loss: 0.6537853479385376
Iteration #4855 loss: 0.5487487316131592
Iteration #4860 loss: 0.5281125903129578
Iteration #4865 loss: 0.47028231620788574
Iteration #4870 loss: 0.4030155539512634
Iteration #4875 loss: 0.510187566280365
Iteration #4880 loss: 0.4896145462989807
Iteration #4885 loss: 0.497581422328949
Iteration #4890 loss: 0.47545307874679565
Iteration #4895 loss: 0.42291533946990967
Iteration #4900 loss: 0.6241183280944824
Iteration #4905 loss: 0.5107812881469727
Iteration #4910 loss: 0.4081818461418152
Iteration #4915 loss: 0.6057348847389221
Iteration #4920 loss: 0.5380465388298035
Iteration #4925 loss: 0.47387969493865967
Iteration #4930 loss: 0.3731146454811096
Epoch #28 Training loss: 0.5624700919670217
Iteration #4935 loss: 0.6368793845176697
Iteration #4940 loss: 0.5087727308273315
Iteration #4945 loss: 0.6826996803283691
Iteration #4950 loss: 0.6750732660293579
Iteration #4955 loss: 0.5726446509361267
Iteration #4960 loss: 0.6061780452728271
Iteration #4965 loss: 0.6169993877410889
Iteration #4970 loss: 0.608057975769043
Iteration #4975 loss: 0.6932395100593567
Iteration #4980 loss: 0.7028845548629761
Iteration #4985 loss: 0.6367756128311157
Iteration #4990 loss: 0.647594690322876
Iteration #4995 loss: 0.5995444059371948
Iteration #5000 loss: 0.6807612180709839
Iteration #5005 loss: 0.6337460875511169
Iteration #5010 loss: 0.44915688037872314
Iteration #5015 loss: 0.3587679862976074
Iteration #5020 loss: 0.6605708003044128
Iteration #5025 loss: 0.5597870349884033
Iteration #5030 loss: 0.5053372383117676
Iteration #5035 loss: 0.4716512858867645
Iteration #5040 loss: 0.40167659521102905
Iteration #5045 loss: 0.518319845199585
Iteration #5050 loss: 0.4835827946662903
Iteration #5055 loss: 0.5075440406799316
Iteration #5060 loss: 0.47040945291519165
Iteration #5065 loss: 0.42673319578170776
Iteration #5070 loss: 0.6221340894699097
Iteration #5075 loss: 0.5218715667724609
Iteration #5080 loss: 0.4083383083343506
Iteration #5085 loss: 0.5980242490768433
Iteration #5090 loss: 0.5408445000648499
Iteration #5095 loss: 0.4890855848789215
Iteration #5100 loss: 0.3514271676540375
Epoch #29 Training loss: 0.562034196362776
Iteration #5105 loss: 0.609163224697113
Iteration #5110 loss: 0.5105597972869873
Iteration #5115 loss: 0.6786496639251709
Iteration #5120 loss: 0.6791761517524719
Iteration #5125 loss: 0.5851336717605591
Iteration #5130 loss: 0.5937363505363464
Iteration #5135 loss: 0.5991559028625488
Iteration #5140 loss: 0.6124922037124634
Iteration #5145 loss: 0.688988983631134
Iteration #5150 loss: 0.6979320049285889
Iteration #5155 loss: 0.6372388005256653
Iteration #5160 loss: 0.630091667175293
Iteration #5165 loss: 0.6383805871009827
Iteration #5170 loss: 0.6486321687698364
Iteration #5175 loss: 0.6431553363800049
Iteration #5180 loss: 0.4486856460571289
Iteration #5185 loss: 0.3369530737400055
Iteration #5190 loss: 0.6527387499809265
Iteration #5195 loss: 0.5420494079589844
Iteration #5200 loss: 0.5108123421669006
Iteration #5205 loss: 0.47280773520469666
Iteration #5210 loss: 0.3953382968902588
Iteration #5215 loss: 0.48772764205932617
Iteration #5220 loss: 0.47677648067474365
Iteration #5225 loss: 0.5075943470001221
Iteration #5230 loss: 0.4749329090118408
Iteration #5235 loss: 0.41262051463127136
Iteration #5240 loss: 0.6202907562255859
Iteration #5245 loss: 0.5143914222717285
Iteration #5250 loss: 0.3989224433898926
Iteration #5255 loss: 0.6043974161148071
Iteration #5260 loss: 0.5490065217018127
Iteration #5265 loss: 0.4685484766960144
Iteration #5270 loss: 0.3541310131549835
Epoch #30 Training loss: 0.5576906170915155
Iteration #5275 loss: 0.5971919298171997
Iteration #5280 loss: 0.5186724066734314
Iteration #5285 loss: 0.6657174825668335
Iteration #5290 loss: 0.6717036962509155
Iteration #5295 loss: 0.5849289894104004
Iteration #5300 loss: 0.5889859795570374
Iteration #5305 loss: 0.6113077402114868
Iteration #5310 loss: 0.5856719017028809
Iteration #5315 loss: 0.689710795879364
Iteration #5320 loss: 0.6971527338027954
Iteration #5325 loss: 0.6163325905799866
Iteration #5330 loss: 0.6153918504714966
Iteration #5335 loss: 0.6034448146820068
Iteration #5340 loss: 0.6623709797859192
Iteration #5345 loss: 0.6152409315109253
Iteration #5350 loss: 0.45169001817703247
Iteration #5355 loss: 0.32683950662612915
Iteration #5360 loss: 0.6845514178276062
Iteration #5365 loss: 0.5585224032402039
Iteration #5370 loss: 0.5039058923721313
Iteration #5375 loss: 0.4564065933227539
Iteration #5380 loss: 0.3943209648132324
Iteration #5385 loss: 0.5147232413291931
Iteration #5390 loss: 0.482544481754303
Iteration #5395 loss: 0.504531741142273
Iteration #5400 loss: 0.48618364334106445
Iteration #5405 loss: 0.42778486013412476
Iteration #5410 loss: 0.6002680659294128
Iteration #5415 loss: 0.5200170874595642
Iteration #5420 loss: 0.3963013291358948
Iteration #5425 loss: 0.6040048003196716
Iteration #5430 loss: 0.5467652082443237
Iteration #5435 loss: 0.4653555154800415
Iteration #5440 loss: 0.33619940280914307
Epoch #31 Training loss: 0.5550791356493445
Iteration #5445 loss: 0.6007713079452515
Iteration #5450 loss: 0.522273063659668
Iteration #5455 loss: 0.6942497491836548
Iteration #5460 loss: 0.6622192859649658
Iteration #5465 loss: 0.5656757950782776
Iteration #5470 loss: 0.6034751534461975
Iteration #5475 loss: 0.5975146293640137
Iteration #5480 loss: 0.6023322939872742
Iteration #5485 loss: 0.678009033203125
Iteration #5490 loss: 0.698407769203186
Iteration #5495 loss: 0.6432076096534729
Iteration #5500 loss: 0.6317496299743652
Iteration #5505 loss: 0.6119803786277771
Iteration #5510 loss: 0.6631731986999512
Iteration #5515 loss: 0.6272624731063843
Iteration #5520 loss: 0.44103217124938965
Iteration #5525 loss: 0.3317390978336334
Iteration #5530 loss: 0.6662157773971558
Iteration #5535 loss: 0.548047661781311
Iteration #5540 loss: 0.5200024843215942
Iteration #5545 loss: 0.4601508677005768
Iteration #5550 loss: 0.3993895351886749
Iteration #5555 loss: 0.5027984380722046
Iteration #5560 loss: 0.47094160318374634
Iteration #5565 loss: 0.5035561919212341
Iteration #5570 loss: 0.47602489590644836
Iteration #5575 loss: 0.42446738481521606
Iteration #5580 loss: 0.5931733846664429
Iteration #5585 loss: 0.512607753276825
Iteration #5590 loss: 0.3934827446937561
Iteration #5595 loss: 0.6093473434448242
Iteration #5600 loss: 0.5219500064849854
Iteration #5605 loss: 0.4669395685195923
Iteration #5610 loss: 0.3598979115486145
Epoch #32 Training loss: 0.5546814331237008
Iteration #5615 loss: 0.6081949472427368
Iteration #5620 loss: 0.5121543407440186
Iteration #5625 loss: 0.6875790357589722
Iteration #5630 loss: 0.6853442192077637
Iteration #5635 loss: 0.5672569274902344
Iteration #5640 loss: 0.5898237228393555
Iteration #5645 loss: 0.6007352471351624
Iteration #5650 loss: 0.595452606678009
Iteration #5655 loss: 0.6749524474143982
Iteration #5660 loss: 0.6817225217819214
Iteration #5665 loss: 0.6066798567771912
Iteration #5670 loss: 0.6327530145645142
Iteration #5675 loss: 0.595483660697937
Iteration #5680 loss: 0.6715487837791443
Iteration #5685 loss: 0.6302157044410706
Iteration #5690 loss: 0.4342670440673828
Iteration #5695 loss: 0.3353040814399719
Iteration #5700 loss: 0.6584279537200928
Iteration #5705 loss: 0.5398328304290771
Iteration #5710 loss: 0.5094370245933533
Iteration #5715 loss: 0.4731406569480896
Iteration #5720 loss: 0.3918221592903137
Iteration #5725 loss: 0.5141702890396118
Iteration #5730 loss: 0.4960101246833801
Iteration #5735 loss: 0.5031151175498962
Iteration #5740 loss: 0.4850323498249054
Iteration #5745 loss: 0.4270007312297821
Iteration #5750 loss: 0.6005330681800842
Iteration #5755 loss: 0.4996865689754486
Iteration #5760 loss: 0.38746893405914307
Iteration #5765 loss: 0.5901851654052734
Iteration #5770 loss: 0.558769702911377
Iteration #5775 loss: 0.4675251245498657
Iteration #5780 loss: 0.3660585284233093
Epoch #33 Training loss: 0.553273897135959
Iteration #5785 loss: 0.6046286821365356
Iteration #5790 loss: 0.5138096809387207
Iteration #5795 loss: 0.673579216003418
Iteration #5800 loss: 0.6758084297180176
Iteration #5805 loss: 0.5610426664352417
Iteration #5810 loss: 0.5862125158309937
Iteration #5815 loss: 0.5844387412071228
Iteration #5820 loss: 0.5836464166641235
Iteration #5825 loss: 0.6636037826538086
Iteration #5830 loss: 0.6913807988166809
Iteration #5835 loss: 0.6141131520271301
Iteration #5840 loss: 0.611696183681488
Iteration #5845 loss: 0.6238139867782593
Iteration #5850 loss: 0.6613016128540039
Iteration #5855 loss: 0.6303969621658325
Iteration #5860 loss: 0.443263441324234
Iteration #5865 loss: 0.3377532362937927
Iteration #5870 loss: 0.6641832590103149
Iteration #5875 loss: 0.529937207698822
Iteration #5880 loss: 0.5070111751556396
Iteration #5885 loss: 0.4607165455818176
Iteration #5890 loss: 0.38790297508239746
Iteration #5895 loss: 0.5018811225891113
Iteration #5900 loss: 0.4671899080276489
Iteration #5905 loss: 0.4930647015571594
Iteration #5910 loss: 0.4750245213508606
Iteration #5915 loss: 0.4107542037963867
Iteration #5920 loss: 0.5947084426879883
Iteration #5925 loss: 0.49495255947113037
Iteration #5930 loss: 0.4055767059326172
Iteration #5935 loss: 0.602829098701477
Iteration #5940 loss: 0.5267717838287354
Iteration #5945 loss: 0.46017831563949585
Iteration #5950 loss: 0.3593278229236603
Epoch #34 Training loss: 0.5510010636904661
Iteration #5955 loss: 0.5882525444030762
Iteration #5960 loss: 0.5283748507499695
Iteration #5965 loss: 0.6727389693260193
Iteration #5970 loss: 0.651147186756134
Iteration #5975 loss: 0.5634158253669739
Iteration #5980 loss: 0.5746326446533203
Iteration #5985 loss: 0.5912898778915405
Iteration #5990 loss: 0.5987427234649658
Iteration #5995 loss: 0.6737269163131714
Iteration #6000 loss: 0.6740604043006897
Iteration #6005 loss: 0.6297762989997864
Iteration #6010 loss: 0.6176551580429077
Iteration #6015 loss: 0.6050763726234436
Iteration #6020 loss: 0.6583079099655151
Iteration #6025 loss: 0.6167184114456177
Iteration #6030 loss: 0.4274400770664215
Iteration #6035 loss: 0.34047985076904297
Iteration #6040 loss: 0.6437259912490845
Iteration #6045 loss: 0.5351918935775757
Iteration #6050 loss: 0.5065265893936157
Iteration #6055 loss: 0.4598114490509033
Iteration #6060 loss: 0.3877541422843933
Iteration #6065 loss: 0.49464839696884155
Iteration #6070 loss: 0.4802311956882477
Iteration #6075 loss: 0.47490328550338745
Iteration #6080 loss: 0.4528312683105469
Iteration #6085 loss: 0.42690110206604004
Iteration #6090 loss: 0.6139158010482788
Iteration #6095 loss: 0.5254956483840942
Iteration #6100 loss: 0.39419659972190857
Iteration #6105 loss: 0.6023670434951782
Iteration #6110 loss: 0.5257853269577026
Iteration #6115 loss: 0.4657747149467468
Iteration #6120 loss: 0.371905654668808
Epoch #35 Training loss: 0.5502336619531407
Iteration #6125 loss: 0.5746950507164001
Iteration #6130 loss: 0.5103837847709656
Iteration #6135 loss: 0.6574355363845825
Iteration #6140 loss: 0.6623837351799011
Iteration #6145 loss: 0.5598529577255249
Iteration #6150 loss: 0.5844941139221191
Iteration #6155 loss: 0.5931184887886047
Iteration #6160 loss: 0.5957332849502563
Iteration #6165 loss: 0.6551552414894104
Iteration #6170 loss: 0.6878060102462769
Iteration #6175 loss: 0.636025071144104
Iteration #6180 loss: 0.6220853328704834
Iteration #6185 loss: 0.6148825883865356
Iteration #6190 loss: 0.6528700590133667
Iteration #6195 loss: 0.631391167640686
Iteration #6200 loss: 0.44278913736343384
Iteration #6205 loss: 0.33360570669174194
Iteration #6210 loss: 0.6599276065826416
Iteration #6215 loss: 0.5256317257881165
Iteration #6220 loss: 0.5035945177078247
Iteration #6225 loss: 0.45344477891921997
Iteration #6230 loss: 0.3913058042526245
Iteration #6235 loss: 0.5107467770576477
Iteration #6240 loss: 0.4702857732772827
Iteration #6245 loss: 0.485398530960083
Iteration #6250 loss: 0.47534212470054626
Iteration #6255 loss: 0.40351828932762146
Iteration #6260 loss: 0.5979077816009521
Iteration #6265 loss: 0.502966046333313
Iteration #6270 loss: 0.37814339995384216
Iteration #6275 loss: 0.5966023206710815
Iteration #6280 loss: 0.5351322293281555
Iteration #6285 loss: 0.4538731575012207
Iteration #6290 loss: 0.3352181613445282
Epoch #36 Training loss: 0.5475810341975268
Iteration #6295 loss: 0.6016269326210022
Iteration #6300 loss: 0.5014376044273376
Iteration #6305 loss: 0.6516566872596741
Iteration #6310 loss: 0.6690232753753662
Iteration #6315 loss: 0.547010064125061
Iteration #6320 loss: 0.5926489233970642
Iteration #6325 loss: 0.5827874541282654
Iteration #6330 loss: 0.5837697982788086
Iteration #6335 loss: 0.6769706606864929
Iteration #6340 loss: 0.6717331409454346
Iteration #6345 loss: 0.6337270736694336
Iteration #6350 loss: 0.609274685382843
Iteration #6355 loss: 0.6051896810531616
Iteration #6360 loss: 0.6684447526931763
Iteration #6365 loss: 0.6124661564826965
Iteration #6370 loss: 0.4249536693096161
Iteration #6375 loss: 0.3275071680545807
Iteration #6380 loss: 0.6528612375259399
Iteration #6385 loss: 0.5389614105224609
Iteration #6390 loss: 0.5118939876556396
Iteration #6395 loss: 0.4579430818557739
Iteration #6400 loss: 0.3953871726989746
Iteration #6405 loss: 0.5065106749534607
Iteration #6410 loss: 0.4746728539466858
Iteration #6415 loss: 0.4677846431732178
Iteration #6420 loss: 0.46268871426582336
Iteration #6425 loss: 0.40293973684310913
Iteration #6430 loss: 0.6053124070167542
Iteration #6435 loss: 0.4990136921405792
Iteration #6440 loss: 0.39924532175064087
Iteration #6445 loss: 0.6068685054779053
Iteration #6450 loss: 0.5275185704231262
Iteration #6455 loss: 0.4457882046699524
Iteration #6460 loss: 0.32714372873306274
Epoch #37 Training loss: 0.5454716012758367
Iteration #6465 loss: 0.5764243006706238
Iteration #6470 loss: 0.5176142454147339
Iteration #6475 loss: 0.6553646922111511
Iteration #6480 loss: 0.6583179235458374
Iteration #6485 loss: 0.5534619092941284
Iteration #6490 loss: 0.5897312164306641
Iteration #6495 loss: 0.5858542919158936
Iteration #6500 loss: 0.5883327722549438
Iteration #6505 loss: 0.6674747467041016
Iteration #6510 loss: 0.6690672636032104
Iteration #6515 loss: 0.6249125003814697
Iteration #6520 loss: 0.6142781972885132
Iteration #6525 loss: 0.6160182952880859
Iteration #6530 loss: 0.6567860245704651
Iteration #6535 loss: 0.6213295459747314
Iteration #6540 loss: 0.44963550567626953
Iteration #6545 loss: 0.3300744295120239
Iteration #6550 loss: 0.6621325016021729
Iteration #6555 loss: 0.5357098579406738
Iteration #6560 loss: 0.49741464853286743
Iteration #6565 loss: 0.4623994827270508
Iteration #6570 loss: 0.38345399498939514
Iteration #6575 loss: 0.5008888244628906
Iteration #6580 loss: 0.47758156061172485
Iteration #6585 loss: 0.4844816327095032
Iteration #6590 loss: 0.46451494097709656
Iteration #6595 loss: 0.40403878688812256
Iteration #6600 loss: 0.5910195112228394
Iteration #6605 loss: 0.5026453733444214
Iteration #6610 loss: 0.3790139853954315
Iteration #6615 loss: 0.5889707207679749
Iteration #6620 loss: 0.5411244034767151
Iteration #6625 loss: 0.46032094955444336
Iteration #6630 loss: 0.3611314296722412
Epoch #38 Training loss: 0.5439818494460162
Iteration #6635 loss: 0.5850328207015991
Iteration #6640 loss: 0.5144485831260681
Iteration #6645 loss: 0.6396650671958923
Iteration #6650 loss: 0.6439571380615234
Iteration #6655 loss: 0.5593317747116089
Iteration #6660 loss: 0.5736328363418579
Iteration #6665 loss: 0.5935861468315125
Iteration #6670 loss: 0.5960708856582642
Iteration #6675 loss: 0.6560519337654114
Iteration #6680 loss: 0.6805891394615173
Iteration #6685 loss: 0.6311014890670776
Iteration #6690 loss: 0.6247995495796204
Iteration #6695 loss: 0.59061598777771
Iteration #6700 loss: 0.6677542924880981
Iteration #6705 loss: 0.6209272146224976
Iteration #6710 loss: 0.43625855445861816
Iteration #6715 loss: 0.31839656829833984
Iteration #6720 loss: 0.6396898031234741
Iteration #6725 loss: 0.5379288792610168
Iteration #6730 loss: 0.5088176131248474
Iteration #6735 loss: 0.45511531829833984
Iteration #6740 loss: 0.3857656419277191
Iteration #6745 loss: 0.49611175060272217
Iteration #6750 loss: 0.46763864159584045
Iteration #6755 loss: 0.4775698184967041
Iteration #6760 loss: 0.45969849824905396
Iteration #6765 loss: 0.39954954385757446
Iteration #6770 loss: 0.5962347984313965
Iteration #6775 loss: 0.4923708736896515
Iteration #6780 loss: 0.3868951201438904
Iteration #6785 loss: 0.5899657607078552
Iteration #6790 loss: 0.5156567096710205
Iteration #6795 loss: 0.45296818017959595
Iteration #6800 loss: 0.349409282207489
Epoch #39 Training loss: 0.5425000623745077
Iteration #6805 loss: 0.5688286423683167
Iteration #6810 loss: 0.5074288845062256
Iteration #6815 loss: 0.6456656455993652
Iteration #6820 loss: 0.6618245840072632
Iteration #6825 loss: 0.5437629818916321
Iteration #6830 loss: 0.5758886337280273
Iteration #6835 loss: 0.5847722291946411
Iteration #6840 loss: 0.5819434523582458
Iteration #6845 loss: 0.660026490688324
Iteration #6850 loss: 0.6709080934524536
Iteration #6855 loss: 0.6203152537345886
Iteration #6860 loss: 0.6090590357780457
Iteration #6865 loss: 0.6150091886520386
Iteration #6870 loss: 0.6704537272453308
Iteration #6875 loss: 0.6288144588470459
Iteration #6880 loss: 0.4270159602165222
Iteration #6885 loss: 0.33739662170410156
Iteration #6890 loss: 0.6529949307441711
Iteration #6895 loss: 0.5262018442153931
Iteration #6900 loss: 0.48319005966186523
Iteration #6905 loss: 0.4601116180419922
Iteration #6910 loss: 0.38679343461990356
Iteration #6915 loss: 0.48922199010849
Iteration #6920 loss: 0.46042245626449585
Iteration #6925 loss: 0.4675895869731903
Iteration #6930 loss: 0.469561904668808
Iteration #6935 loss: 0.4093859791755676
Iteration #6940 loss: 0.5978196263313293
Iteration #6945 loss: 0.4988445043563843
Iteration #6950 loss: 0.37677156925201416
Iteration #6955 loss: 0.6072932481765747
Iteration #6960 loss: 0.5055105090141296
Iteration #6965 loss: 0.45817217230796814
Iteration #6970 loss: 0.3214856684207916
Epoch #40 Training loss: 0.5410294082234888
Iteration #6975 loss: 0.5811247825622559
Iteration #6980 loss: 0.48953384160995483
Iteration #6985 loss: 0.6455821990966797
Iteration #6990 loss: 0.6532644033432007
Iteration #6995 loss: 0.5438980460166931
Iteration #7000 loss: 0.5774840712547302
Iteration #7005 loss: 0.5774433612823486
Iteration #7010 loss: 0.5915617942810059
Iteration #7015 loss: 0.6493631601333618
Iteration #7020 loss: 0.6746887564659119
Iteration #7025 loss: 0.6211921572685242
Iteration #7030 loss: 0.6052072048187256
Iteration #7035 loss: 0.6023593544960022
Iteration #7040 loss: 0.641700029373169
Iteration #7045 loss: 0.6212445497512817
Iteration #7050 loss: 0.41633063554763794
Iteration #7055 loss: 0.32896751165390015
Iteration #7060 loss: 0.6277333498001099
Iteration #7065 loss: 0.5153282880783081
Iteration #7070 loss: 0.48809128999710083
Iteration #7075 loss: 0.4516211748123169
Iteration #7080 loss: 0.37975913286209106
Iteration #7085 loss: 0.5012174248695374
Iteration #7090 loss: 0.45967692136764526
Iteration #7095 loss: 0.46590811014175415
Iteration #7100 loss: 0.4818741977214813
Iteration #7105 loss: 0.4004283547401428
Iteration #7110 loss: 0.5950647592544556
Iteration #7115 loss: 0.4717515707015991
Iteration #7120 loss: 0.3842771351337433
Iteration #7125 loss: 0.5845574736595154
Iteration #7130 loss: 0.5206190943717957
Iteration #7135 loss: 0.46960899233818054
Iteration #7140 loss: 0.3114866018295288
Epoch #41 Training loss: 0.5382471915553598
Iteration #7145 loss: 0.581139326095581
Iteration #7150 loss: 0.49351930618286133
Iteration #7155 loss: 0.6309309005737305
Iteration #7160 loss: 0.6386603116989136
Iteration #7165 loss: 0.5567766427993774
Iteration #7170 loss: 0.5696494579315186
Iteration #7175 loss: 0.585808277130127
Iteration #7180 loss: 0.5740971565246582
Iteration #7185 loss: 0.6579840183258057
Iteration #7190 loss: 0.6655029058456421
Iteration #7195 loss: 0.60406494140625
Iteration #7200 loss: 0.6004477739334106
Iteration #7205 loss: 0.6073015332221985
Iteration #7210 loss: 0.6439281702041626
Iteration #7215 loss: 0.6182010173797607
Iteration #7220 loss: 0.4333459734916687
Iteration #7225 loss: 0.32088515162467957
Iteration #7230 loss: 0.620651364326477
Iteration #7235 loss: 0.5350042581558228
Iteration #7240 loss: 0.501232385635376
Iteration #7245 loss: 0.44894087314605713
Iteration #7250 loss: 0.38515910506248474
Iteration #7255 loss: 0.5014046430587769
Iteration #7260 loss: 0.46388429403305054
Iteration #7265 loss: 0.4662955403327942
Iteration #7270 loss: 0.4592285752296448
Iteration #7275 loss: 0.4037606716156006
Iteration #7280 loss: 0.5778068900108337
Iteration #7285 loss: 0.4866945445537567
Iteration #7290 loss: 0.37765687704086304
Iteration #7295 loss: 0.5642184019088745
Iteration #7300 loss: 0.522981584072113
Iteration #7305 loss: 0.46096891164779663
Iteration #7310 loss: 0.3222586214542389
Epoch #42 Training loss: 0.5376068837502423
Iteration #7315 loss: 0.5823526382446289
Iteration #7320 loss: 0.5212920904159546
Iteration #7325 loss: 0.6325709819793701
Iteration #7330 loss: 0.6434577703475952
Iteration #7335 loss: 0.5464405417442322
Iteration #7340 loss: 0.5633983612060547
Iteration #7345 loss: 0.5872150659561157
Iteration #7350 loss: 0.5819635391235352
Iteration #7355 loss: 0.6621779203414917
Iteration #7360 loss: 0.6753870248794556
Iteration #7365 loss: 0.593227744102478
Iteration #7370 loss: 0.6101217269897461
Iteration #7375 loss: 0.6015536785125732
Iteration #7380 loss: 0.6497771143913269
Iteration #7385 loss: 0.6443866491317749
Iteration #7390 loss: 0.4227412939071655
Iteration #7395 loss: 0.31273096799850464
Iteration #7400 loss: 0.6394639015197754
Iteration #7405 loss: 0.5152884125709534
Iteration #7410 loss: 0.48274868726730347
Iteration #7415 loss: 0.44978880882263184
Iteration #7420 loss: 0.3816834092140198
Iteration #7425 loss: 0.499942421913147
Iteration #7430 loss: 0.45886147022247314
Iteration #7435 loss: 0.4657576382160187
Iteration #7440 loss: 0.4461294114589691
Iteration #7445 loss: 0.41816598176956177
Iteration #7450 loss: 0.584388792514801
Iteration #7455 loss: 0.48556557297706604
Iteration #7460 loss: 0.36668848991394043
Iteration #7465 loss: 0.5994728207588196
Iteration #7470 loss: 0.5057557821273804
Iteration #7475 loss: 0.45995765924453735
Iteration #7480 loss: 0.3255199193954468
Epoch #43 Training loss: 0.5335144037709517
Iteration #7485 loss: 0.5688451528549194
Iteration #7490 loss: 0.4787500500679016
Iteration #7495 loss: 0.6369446516036987
Iteration #7500 loss: 0.6522355675697327
Iteration #7505 loss: 0.556723952293396
Iteration #7510 loss: 0.5844135284423828
Iteration #7515 loss: 0.5831766128540039
Iteration #7520 loss: 0.5866360068321228
Iteration #7525 loss: 0.6498674154281616
Iteration #7530 loss: 0.6787813305854797
Iteration #7535 loss: 0.6156344413757324
Iteration #7540 loss: 0.5977586507797241
Iteration #7545 loss: 0.6115418672561646
Iteration #7550 loss: 0.6525677442550659
Iteration #7555 loss: 0.6114023327827454
Iteration #7560 loss: 0.40570878982543945
Iteration #7565 loss: 0.32515090703964233
Iteration #7570 loss: 0.6378976106643677
Iteration #7575 loss: 0.5140864849090576
Iteration #7580 loss: 0.497171014547348
Iteration #7585 loss: 0.44591447710990906
Iteration #7590 loss: 0.3863567113876343
Iteration #7595 loss: 0.48740583658218384
Iteration #7600 loss: 0.45168209075927734
Iteration #7605 loss: 0.4692075252532959
Iteration #7610 loss: 0.46369826793670654
Iteration #7615 loss: 0.40073853731155396
Iteration #7620 loss: 0.6004183292388916
Iteration #7625 loss: 0.4802680015563965
Iteration #7630 loss: 0.3630821108818054
Iteration #7635 loss: 0.6144298315048218
Iteration #7640 loss: 0.5173488259315491
Iteration #7645 loss: 0.45460450649261475
Iteration #7650 loss: 0.31689661741256714
Epoch #44 Training loss: 0.5335153230849434
Iteration #7655 loss: 0.5660495758056641
Iteration #7660 loss: 0.4999126195907593
Iteration #7665 loss: 0.6186482310295105
Iteration #7670 loss: 0.6337127685546875
Iteration #7675 loss: 0.5431151390075684
Iteration #7680 loss: 0.5563636422157288
Iteration #7685 loss: 0.5853712558746338
Iteration #7690 loss: 0.584969162940979
Iteration #7695 loss: 0.6306881904602051
Iteration #7700 loss: 0.6793490052223206
Iteration #7705 loss: 0.5982868671417236
Iteration #7710 loss: 0.5972076654434204
Iteration #7715 loss: 0.5987562537193298
Iteration #7720 loss: 0.6491091251373291
Iteration #7725 loss: 0.6189561486244202
Iteration #7730 loss: 0.4262429475784302
Iteration #7735 loss: 0.32045334577560425
Iteration #7740 loss: 0.6587694883346558
Iteration #7745 loss: 0.5148874521255493
Iteration #7750 loss: 0.472839891910553
Iteration #7755 loss: 0.43815773725509644
Iteration #7760 loss: 0.3848099112510681
Iteration #7765 loss: 0.48455411195755005
Iteration #7770 loss: 0.4561566710472107
Iteration #7775 loss: 0.4828474521636963
Iteration #7780 loss: 0.45221567153930664
Iteration #7785 loss: 0.398164838552475
Iteration #7790 loss: 0.5881132483482361
Iteration #7795 loss: 0.45669353008270264
Iteration #7800 loss: 0.37625908851623535
Iteration #7805 loss: 0.5891398191452026
Iteration #7810 loss: 0.5148085355758667
Iteration #7815 loss: 0.4702170193195343
Iteration #7820 loss: 0.3197143077850342
Epoch #45 Training loss: 0.5310847021201077
Iteration #7825 loss: 0.5736504793167114
Iteration #7830 loss: 0.5066276788711548
Iteration #7835 loss: 0.628716230392456
Iteration #7840 loss: 0.6485607624053955
Iteration #7845 loss: 0.5389740467071533
Iteration #7850 loss: 0.5618031024932861
Iteration #7855 loss: 0.5665779113769531
Iteration #7860 loss: 0.5679156184196472
Iteration #7865 loss: 0.6578425765037537
Iteration #7870 loss: 0.6579253673553467
Iteration #7875 loss: 0.6018168926239014
Iteration #7880 loss: 0.6141910552978516
Iteration #7885 loss: 0.6005644798278809
Iteration #7890 loss: 0.6349718570709229
Iteration #7895 loss: 0.625931441783905
Iteration #7900 loss: 0.4176543056964874
Iteration #7905 loss: 0.31456881761550903
Iteration #7910 loss: 0.6388223171234131
Iteration #7915 loss: 0.5216169953346252
Iteration #7920 loss: 0.49428075551986694
Iteration #7925 loss: 0.4570595622062683
Iteration #7930 loss: 0.38755661249160767
Iteration #7935 loss: 0.49292969703674316
Iteration #7940 loss: 0.46205002069473267
Iteration #7945 loss: 0.4707518219947815
Iteration #7950 loss: 0.45300373435020447
Iteration #7955 loss: 0.39714205265045166
Iteration #7960 loss: 0.5702751278877258
Iteration #7965 loss: 0.4537399411201477
Iteration #7970 loss: 0.36018404364585876
Iteration #7975 loss: 0.5921363830566406
Iteration #7980 loss: 0.5110877156257629
Iteration #7985 loss: 0.459060937166214
Iteration #7990 loss: 0.28881946206092834
Epoch #46 Training loss: 0.5296589265851414
Iteration #7995 loss: 0.5779783129692078
Iteration #8000 loss: 0.4944987893104553
Iteration #8005 loss: 0.6204062700271606
Iteration #8010 loss: 0.6493402123451233
Iteration #8015 loss: 0.5490519404411316
Iteration #8020 loss: 0.5745574235916138
Iteration #8025 loss: 0.5781041979789734
Iteration #8030 loss: 0.5792919397354126
Iteration #8035 loss: 0.6330462694168091
Iteration #8040 loss: 0.6653631925582886
Iteration #8045 loss: 0.6027170419692993
Iteration #8050 loss: 0.6082764863967896
Iteration #8055 loss: 0.5869431495666504
Iteration #8060 loss: 0.6410129070281982
Iteration #8065 loss: 0.6065175533294678
Iteration #8070 loss: 0.394855260848999
Iteration #8075 loss: 0.3091849982738495
Iteration #8080 loss: 0.6441857814788818
Iteration #8085 loss: 0.53621906042099
Iteration #8090 loss: 0.488286554813385
Iteration #8095 loss: 0.4388529360294342
Iteration #8100 loss: 0.3832550048828125
Iteration #8105 loss: 0.5027347207069397
Iteration #8110 loss: 0.4589974284172058
Iteration #8115 loss: 0.4720268249511719
Iteration #8120 loss: 0.4526069164276123
Iteration #8125 loss: 0.41722217202186584
Iteration #8130 loss: 0.573925793170929
Iteration #8135 loss: 0.486520916223526
Iteration #8140 loss: 0.364229679107666
Iteration #8145 loss: 0.5762368440628052
Iteration #8150 loss: 0.4949778616428375
Iteration #8155 loss: 0.44845154881477356
Iteration #8160 loss: 0.301691472530365
Epoch #47 Training loss: 0.5277992742903093
Iteration #8165 loss: 0.5588700175285339
Iteration #8170 loss: 0.49140337109565735
Iteration #8175 loss: 0.6248468160629272
Iteration #8180 loss: 0.6355060338973999
Iteration #8185 loss: 0.5416816473007202
Iteration #8190 loss: 0.5685688853263855
Iteration #8195 loss: 0.5864927172660828
Iteration #8200 loss: 0.5690277218818665
Iteration #8205 loss: 0.6570857763290405
Iteration #8210 loss: 0.6629115343093872
Iteration #8215 loss: 0.592157781124115
Iteration #8220 loss: 0.5987273454666138
Iteration #8225 loss: 0.6028716564178467
Iteration #8230 loss: 0.6451908349990845
Iteration #8235 loss: 0.6118444800376892
Iteration #8240 loss: 0.4029623568058014
Iteration #8245 loss: 0.31074222922325134
Iteration #8250 loss: 0.6103643774986267
Iteration #8255 loss: 0.5096998810768127
Iteration #8260 loss: 0.5008267164230347
Iteration #8265 loss: 0.4422910511493683
Iteration #8270 loss: 0.38860008120536804
Iteration #8275 loss: 0.4871998429298401
Iteration #8280 loss: 0.44738084077835083
Iteration #8285 loss: 0.4617329239845276
Iteration #8290 loss: 0.4545786380767822
Iteration #8295 loss: 0.40126359462738037
Iteration #8300 loss: 0.5756601095199585
Iteration #8305 loss: 0.46063244342803955
Iteration #8310 loss: 0.37327173352241516
Iteration #8315 loss: 0.5956799983978271
Iteration #8320 loss: 0.5111477971076965
Iteration #8325 loss: 0.46538811922073364
Iteration #8330 loss: 0.3072326183319092
Epoch #48 Training loss: 0.5286403126576368
Iteration #8335 loss: 0.5696415901184082
Iteration #8340 loss: 0.47191324830055237
Iteration #8345 loss: 0.6232469081878662
Iteration #8350 loss: 0.630450963973999
Iteration #8355 loss: 0.5475713014602661
Iteration #8360 loss: 0.5617458820343018
Iteration #8365 loss: 0.567586362361908
Iteration #8370 loss: 0.5850539803504944
Iteration #8375 loss: 0.643899142742157
Iteration #8380 loss: 0.6502010226249695
Iteration #8385 loss: 0.6121084690093994
Iteration #8390 loss: 0.6042643189430237
Iteration #8395 loss: 0.5977602601051331
Iteration #8400 loss: 0.6319817304611206
Iteration #8405 loss: 0.6208977699279785
Iteration #8410 loss: 0.3976859450340271
Iteration #8415 loss: 0.31747981905937195
Iteration #8420 loss: 0.6252250075340271
Iteration #8425 loss: 0.5141419172286987
Iteration #8430 loss: 0.4810659885406494
Iteration #8435 loss: 0.4499673843383789
Iteration #8440 loss: 0.37972527742385864
Iteration #8445 loss: 0.48335254192352295
Iteration #8450 loss: 0.44061365723609924
Iteration #8455 loss: 0.45573878288269043
Iteration #8460 loss: 0.44936901330947876
Iteration #8465 loss: 0.3925243020057678
Iteration #8470 loss: 0.5673220157623291
Iteration #8475 loss: 0.4823606014251709
Iteration #8480 loss: 0.35391777753829956
Iteration #8485 loss: 0.5931054353713989
Iteration #8490 loss: 0.5106255412101746
Iteration #8495 loss: 0.4608282446861267
Iteration #8500 loss: 0.3351849615573883
Epoch #49 Training loss: 0.5249468163532369
Iteration #8505 loss: 0.5789564251899719
Iteration #8510 loss: 0.46480119228363037
Iteration #8515 loss: 0.628290057182312
Iteration #8520 loss: 0.6456626057624817
Iteration #8525 loss: 0.5340534448623657
Iteration #8530 loss: 0.5516023635864258
Iteration #8535 loss: 0.5809656381607056
Iteration #8540 loss: 0.5588189363479614
Iteration #8545 loss: 0.6393012404441833
Iteration #8550 loss: 0.650368869304657
Iteration #8555 loss: 0.6095499992370605
Iteration #8560 loss: 0.5890442132949829
Iteration #8565 loss: 0.5817196369171143
Iteration #8570 loss: 0.637671172618866
Iteration #8575 loss: 0.5940027236938477
Iteration #8580 loss: 0.3923856019973755
Iteration #8585 loss: 0.30760160088539124
Iteration #8590 loss: 0.6209996938705444
Iteration #8595 loss: 0.5109546184539795
Iteration #8600 loss: 0.48432835936546326
Iteration #8605 loss: 0.43718814849853516
Iteration #8610 loss: 0.37689000368118286
Iteration #8615 loss: 0.4673750400543213
Iteration #8620 loss: 0.4446294903755188
Iteration #8625 loss: 0.4541915953159332
Iteration #8630 loss: 0.46026161313056946
Iteration #8635 loss: 0.4008845090866089
Iteration #8640 loss: 0.5687121748924255
Iteration #8645 loss: 0.48223963379859924
Iteration #8650 loss: 0.3736635446548462
Iteration #8655 loss: 0.5773109197616577
Iteration #8660 loss: 0.4824966788291931
Iteration #8665 loss: 0.453813374042511
Iteration #8670 loss: 0.2922983467578888
Epoch #50 Training loss: 0.5233034663340624
Iteration #8675 loss: 0.5613146424293518
Iteration #8680 loss: 0.47455331683158875
Iteration #8685 loss: 0.6273383498191833
Iteration #8690 loss: 0.6510791778564453
Iteration #8695 loss: 0.5393390655517578
Iteration #8700 loss: 0.5663118958473206
Iteration #8705 loss: 0.5700345635414124
Iteration #8710 loss: 0.559234619140625
Iteration #8715 loss: 0.6505255699157715
Iteration #8720 loss: 0.653706967830658
Iteration #8725 loss: 0.5902184247970581
Iteration #8730 loss: 0.5982561111450195
Iteration #8735 loss: 0.5753869414329529
Iteration #8740 loss: 0.620897650718689
Iteration #8745 loss: 0.612794041633606
Iteration #8750 loss: 0.3985879123210907
Iteration #8755 loss: 0.3081173300743103
Iteration #8760 loss: 0.6062924861907959
Iteration #8765 loss: 0.4993670880794525
Iteration #8770 loss: 0.4866882562637329
Iteration #8775 loss: 0.4393448233604431
Iteration #8780 loss: 0.38483619689941406
Iteration #8785 loss: 0.48990151286125183
Iteration #8790 loss: 0.4587121605873108
Iteration #8795 loss: 0.4598962068557739
Iteration #8800 loss: 0.4625425338745117
Iteration #8805 loss: 0.39100778102874756
Iteration #8810 loss: 0.5577815175056458
Iteration #8815 loss: 0.4739301800727844
Iteration #8820 loss: 0.35998135805130005
Iteration #8825 loss: 0.5808044672012329
Iteration #8830 loss: 0.5086861848831177
Iteration #8835 loss: 0.44323185086250305
Iteration #8840 loss: 0.3127453327178955
Epoch #51 Training loss: 0.5217083443613614
Iteration #8845 loss: 0.5470074415206909
Iteration #8850 loss: 0.4896848201751709
Iteration #8855 loss: 0.6206960678100586
Iteration #8860 loss: 0.6323944330215454
Iteration #8865 loss: 0.5453652143478394
Iteration #8870 loss: 0.5670081377029419
Iteration #8875 loss: 0.5781192779541016
Iteration #8880 loss: 0.5827935338020325
Iteration #8885 loss: 0.6187660098075867
Iteration #8890 loss: 0.6603920459747314
Iteration #8895 loss: 0.6010648012161255
Iteration #8900 loss: 0.5980987548828125
Iteration #8905 loss: 0.5926178693771362
Iteration #8910 loss: 0.6454949378967285
Iteration #8915 loss: 0.5907383561134338
Iteration #8920 loss: 0.3864072263240814
Iteration #8925 loss: 0.3208093047142029
Iteration #8930 loss: 0.630846381187439
Iteration #8935 loss: 0.518112063407898
Iteration #8940 loss: 0.4676364064216614
Iteration #8945 loss: 0.4567718505859375
Iteration #8950 loss: 0.3756735026836395
Iteration #8955 loss: 0.4847519099712372
Iteration #8960 loss: 0.44766032695770264
Iteration #8965 loss: 0.45500093698501587
Iteration #8970 loss: 0.4460611343383789
Iteration #8975 loss: 0.39143145084381104
Iteration #8980 loss: 0.5592922568321228
Iteration #8985 loss: 0.45037904381752014
Iteration #8990 loss: 0.35878804326057434
Iteration #8995 loss: 0.6092392206192017
Iteration #9000 loss: 0.49808189272880554
Iteration #9005 loss: 0.44669991731643677
Iteration #9010 loss: 0.27906304597854614
Epoch #52 Training loss: 0.5197484056739247
Iteration #9015 loss: 0.5684701800346375
Iteration #9020 loss: 0.4976934492588043
Iteration #9025 loss: 0.6146430969238281
Iteration #9030 loss: 0.6195625066757202
Iteration #9035 loss: 0.5419743061065674
Iteration #9040 loss: 0.5405527353286743
Iteration #9045 loss: 0.5612059235572815
Iteration #9050 loss: 0.5765066146850586
Iteration #9055 loss: 0.6505019068717957
Iteration #9060 loss: 0.6509847640991211
Iteration #9065 loss: 0.5938541293144226
Iteration #9070 loss: 0.5931894779205322
Iteration #9075 loss: 0.5803220272064209
Iteration #9080 loss: 0.6115613579750061
Iteration #9085 loss: 0.6121661067008972
Iteration #9090 loss: 0.38462594151496887
Iteration #9095 loss: 0.31601202487945557
Iteration #9100 loss: 0.6117112636566162
Iteration #9105 loss: 0.5195006132125854
Iteration #9110 loss: 0.4776569604873657
Iteration #9115 loss: 0.4346608817577362
Iteration #9120 loss: 0.3856301009654999
Iteration #9125 loss: 0.47402423620224
Iteration #9130 loss: 0.44145509600639343
Iteration #9135 loss: 0.44553834199905396
Iteration #9140 loss: 0.4470197558403015
Iteration #9145 loss: 0.3899425268173218
Iteration #9150 loss: 0.5511734485626221
Iteration #9155 loss: 0.4454905688762665
Iteration #9160 loss: 0.34310436248779297
Iteration #9165 loss: 0.5575888752937317
Iteration #9170 loss: 0.5009076595306396
Iteration #9175 loss: 0.45180457830429077
Iteration #9180 loss: 0.3152563273906708
Epoch #53 Training loss: 0.5186512217802159
Iteration #9185 loss: 0.5547112822532654
Iteration #9190 loss: 0.4902949929237366
Iteration #9195 loss: 0.6027594804763794
Iteration #9200 loss: 0.6318020224571228
Iteration #9205 loss: 0.5363999605178833
Iteration #9210 loss: 0.5498788952827454
Iteration #9215 loss: 0.5725384950637817
Iteration #9220 loss: 0.5843386650085449
Iteration #9225 loss: 0.6335268020629883
Iteration #9230 loss: 0.6487367153167725
Iteration #9235 loss: 0.5911192297935486
Iteration #9240 loss: 0.5841941833496094
Iteration #9245 loss: 0.5728837251663208
Iteration #9250 loss: 0.637518048286438
Iteration #9255 loss: 0.5955898761749268
Iteration #9260 loss: 0.4013338088989258
Iteration #9265 loss: 0.3051757216453552
Iteration #9270 loss: 0.6125785112380981
Iteration #9275 loss: 0.5179292559623718
Iteration #9280 loss: 0.47817641496658325
Iteration #9285 loss: 0.4341658651828766
Iteration #9290 loss: 0.3759424686431885
Iteration #9295 loss: 0.47571223974227905
Iteration #9300 loss: 0.45547616481781006
Iteration #9305 loss: 0.45545756816864014
Iteration #9310 loss: 0.45029619336128235
Iteration #9315 loss: 0.3987783193588257
Iteration #9320 loss: 0.5706380605697632
Iteration #9325 loss: 0.45084065198898315
Iteration #9330 loss: 0.33708181977272034
Iteration #9335 loss: 0.6023539304733276
Iteration #9340 loss: 0.47565266489982605
Iteration #9345 loss: 0.4331779479980469
Iteration #9350 loss: 0.28093960881233215
Epoch #54 Training loss: 0.5175662096808938
Iteration #9355 loss: 0.5744994878768921
Iteration #9360 loss: 0.4911416172981262
Iteration #9365 loss: 0.6212987899780273
Iteration #9370 loss: 0.6412498950958252
Iteration #9375 loss: 0.5301699638366699
Iteration #9380 loss: 0.5480362176895142
Iteration #9385 loss: 0.5799497365951538
Iteration #9390 loss: 0.5720937252044678
Iteration #9395 loss: 0.6333940029144287
Iteration #9400 loss: 0.6671721339225769
Iteration #9405 loss: 0.6018936634063721
Iteration #9410 loss: 0.5901095271110535
Iteration #9415 loss: 0.5830358862876892
Iteration #9420 loss: 0.6482065916061401
Iteration #9425 loss: 0.5815572142601013
Iteration #9430 loss: 0.37878164649009705
Iteration #9435 loss: 0.30448484420776367
Iteration #9440 loss: 0.6010963916778564
Iteration #9445 loss: 0.5119036436080933
Iteration #9450 loss: 0.46907782554626465
Iteration #9455 loss: 0.4369230270385742
Iteration #9460 loss: 0.37125611305236816
Iteration #9465 loss: 0.47838133573532104
Iteration #9470 loss: 0.44168856739997864
Iteration #9475 loss: 0.44725683331489563
Iteration #9480 loss: 0.4383091330528259
Iteration #9485 loss: 0.4048210382461548
Iteration #9490 loss: 0.5638774633407593
Iteration #9495 loss: 0.46304112672805786
Iteration #9500 loss: 0.35159313678741455
Iteration #9505 loss: 0.5558997988700867
Iteration #9510 loss: 0.4728379249572754
Iteration #9515 loss: 0.45471513271331787
Iteration #9520 loss: 0.28735893964767456
Epoch #55 Training loss: 0.517441011001082
Iteration #9525 loss: 0.5587993860244751
Iteration #9530 loss: 0.491632878780365
Iteration #9535 loss: 0.6103475093841553
Iteration #9540 loss: 0.6203416585922241
Iteration #9545 loss: 0.5392879247665405
Iteration #9550 loss: 0.5637104511260986
Iteration #9555 loss: 0.566612184047699
Iteration #9560 loss: 0.5641958117485046
Iteration #9565 loss: 0.6268616914749146
Iteration #9570 loss: 0.6448593139648438
Iteration #9575 loss: 0.5916112065315247
Iteration #9580 loss: 0.5766618251800537
Iteration #9585 loss: 0.5875601768493652
Iteration #9590 loss: 0.6371375322341919
Iteration #9595 loss: 0.5973849892616272
Iteration #9600 loss: 0.36818671226501465
Iteration #9605 loss: 0.2906099259853363
Iteration #9610 loss: 0.582584798336029
Iteration #9615 loss: 0.5086400508880615
Iteration #9620 loss: 0.4758807420730591
Iteration #9625 loss: 0.44092056155204773
Iteration #9630 loss: 0.3686799108982086
Iteration #9635 loss: 0.4822876751422882
Iteration #9640 loss: 0.4491611123085022
Iteration #9645 loss: 0.45738881826400757
Iteration #9650 loss: 0.456809937953949
Iteration #9655 loss: 0.3971797525882721
Iteration #9660 loss: 0.5479419231414795
Iteration #9665 loss: 0.4593743085861206
Iteration #9670 loss: 0.34220319986343384
Iteration #9675 loss: 0.5749379992485046
Iteration #9680 loss: 0.48375824093818665
Iteration #9685 loss: 0.4461962580680847
Iteration #9690 loss: 0.291392058134079
Epoch #56 Training loss: 0.5151854376582539
Iteration #9695 loss: 0.5805749893188477
Iteration #9700 loss: 0.47115349769592285
Iteration #9705 loss: 0.6091866493225098
Iteration #9710 loss: 0.6281155943870544
Iteration #9715 loss: 0.5226420164108276
Iteration #9720 loss: 0.558690071105957
Iteration #9725 loss: 0.5541553497314453
Iteration #9730 loss: 0.5596757531166077
Iteration #9735 loss: 0.6100786924362183
Iteration #9740 loss: 0.6562728881835938
Iteration #9745 loss: 0.6046292781829834
Iteration #9750 loss: 0.5782840251922607
Iteration #9755 loss: 0.5924065709114075
Iteration #9760 loss: 0.6361199617385864
Iteration #9765 loss: 0.597459077835083
Iteration #9770 loss: 0.37825947999954224
Iteration #9775 loss: 0.3041526675224304
Iteration #9780 loss: 0.6031699180603027
Iteration #9785 loss: 0.5032215118408203
Iteration #9790 loss: 0.4620192050933838
Iteration #9795 loss: 0.43922457098960876
Iteration #9800 loss: 0.3811706602573395
Iteration #9805 loss: 0.4726053774356842
Iteration #9810 loss: 0.4368473291397095
Iteration #9815 loss: 0.45222777128219604
Iteration #9820 loss: 0.4403657913208008
Iteration #9825 loss: 0.38146549463272095
Iteration #9830 loss: 0.5521982312202454
Iteration #9835 loss: 0.4534653127193451
Iteration #9840 loss: 0.34880417585372925
Iteration #9845 loss: 0.5394876003265381
Iteration #9850 loss: 0.46140843629837036
Iteration #9855 loss: 0.46404552459716797
Iteration #9860 loss: 0.2532060146331787
Epoch #57 Training loss: 0.5128740494742113
Iteration #9865 loss: 0.529936671257019
Iteration #9870 loss: 0.47062942385673523
Iteration #9875 loss: 0.5892874598503113
Iteration #9880 loss: 0.6279520988464355
Iteration #9885 loss: 0.5383368730545044
Iteration #9890 loss: 0.570787250995636
Iteration #9895 loss: 0.5629037022590637
Iteration #9900 loss: 0.5727412104606628
Iteration #9905 loss: 0.6242533326148987
Iteration #9910 loss: 0.6443051099777222
Iteration #9915 loss: 0.5885416269302368
Iteration #9920 loss: 0.5760205984115601
Iteration #9925 loss: 0.5900559425354004
Iteration #9930 loss: 0.6361843943595886
Iteration #9935 loss: 0.5890825986862183
Iteration #9940 loss: 0.3702775239944458
Iteration #9945 loss: 0.30632448196411133
Iteration #9950 loss: 0.5899579524993896
Iteration #9955 loss: 0.48262903094291687
Iteration #9960 loss: 0.46297529339790344
Iteration #9965 loss: 0.43237894773483276
Iteration #9970 loss: 0.3668312430381775
Iteration #9975 loss: 0.46831274032592773
Iteration #9980 loss: 0.441677987575531
Iteration #9985 loss: 0.44871750473976135
Iteration #9990 loss: 0.43859827518463135
Iteration #9995 loss: 0.3800757825374603
Iteration #10000 loss: 0.5290213823318481
Iteration #10005 loss: 0.44576019048690796
Iteration #10010 loss: 0.3463839888572693
Iteration #10015 loss: 0.5596171021461487
Iteration #10020 loss: 0.47974833846092224
Iteration #10025 loss: 0.4623432159423828
Iteration #10030 loss: 0.271535187959671
Epoch #58 Training loss: 0.509655550122261
Iteration #10035 loss: 0.5373032689094543
Iteration #10040 loss: 0.4866349697113037
Iteration #10045 loss: 0.5946741700172424
Iteration #10050 loss: 0.6421633958816528
Iteration #10055 loss: 0.5402235984802246
Iteration #10060 loss: 0.5693175792694092
Iteration #10065 loss: 0.5476190447807312
Iteration #10070 loss: 0.5860791206359863
Iteration #10075 loss: 0.6230822205543518
Iteration #10080 loss: 0.6398878693580627
Iteration #10085 loss: 0.5903127193450928
Iteration #10090 loss: 0.5719383955001831
Iteration #10095 loss: 0.5668103098869324
Iteration #10100 loss: 0.6131783723831177
Iteration #10105 loss: 0.5929732322692871
Iteration #10110 loss: 0.3632084131240845
Iteration #10115 loss: 0.2926127314567566
Iteration #10120 loss: 0.5897469520568848
Iteration #10125 loss: 0.5023447871208191
Iteration #10130 loss: 0.47655266523361206
Iteration #10135 loss: 0.43533092737197876
Iteration #10140 loss: 0.3732144832611084
Iteration #10145 loss: 0.4676368534564972
Iteration #10150 loss: 0.4465562403202057
Iteration #10155 loss: 0.4382665157318115
Iteration #10160 loss: 0.43387341499328613
Iteration #10165 loss: 0.3840206563472748
Iteration #10170 loss: 0.5373262166976929
Iteration #10175 loss: 0.45299389958381653
Iteration #10180 loss: 0.3413428068161011
Iteration #10185 loss: 0.5616627335548401
Iteration #10190 loss: 0.46026384830474854
Iteration #10195 loss: 0.4409603476524353
Iteration #10200 loss: 0.2642843723297119
Epoch #59 Training loss: 0.510408745267812
